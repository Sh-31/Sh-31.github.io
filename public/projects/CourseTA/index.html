<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CourseTA | Sherif Ahmed</title><meta name=keywords content="Agentic-System,Langgraph,RAG,LLMs,Human-In-The-Loop,Async,Reflection-agent"><meta name=description content="An agent-based educational system using FastAPI, LangChain, and LangGraph in a microservice architecture. Implemented real-time async endpoints, integrated Whisper for audio/video transcription, and PyMuPDF for PDF parsing. Built a RAG-based QA pipeline with embedding-based retrieval, and AI agents for question generation, summarization, and feedback refinement. Optimized for scalable human-in-the-loop workflows and efficient content transformation across diverse formats."><meta name=author content="Sherif Ahmed"><link rel=canonical href=http://localhost:1313/projects/courseta/><link crossorigin=anonymous href=/assets/css/stylesheet.023a79c09a6253ba6d2a259b4c3db25df8cce828b8b0e2cc06c6675372eb7b43.css integrity="sha256-Ajp5wJpiU7ptKiWbTD2yXfjM6Ci4sOLMBsZnU3Lre0M=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/courseta/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/courseta/"><meta property="og:site_name" content="Sherif Ahmed"><meta property="og:title" content="CourseTA"><meta property="og:description" content="An agent-based educational system using FastAPI, LangChain, and LangGraph in a microservice architecture. Implemented real-time async endpoints, integrated Whisper for audio/video transcription, and PyMuPDF for PDF parsing. Built a RAG-based QA pipeline with embedding-based retrieval, and AI agents for question generation, summarization, and feedback refinement. Optimized for scalable human-in-the-loop workflows and efficient content transformation across diverse formats."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-06-03T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-03T00:00:00+00:00"><meta property="article:tag" content="Agentic-System"><meta property="article:tag" content="Langgraph"><meta property="article:tag" content="RAG"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="Human-in-the-Loop"><meta property="article:tag" content="Async"><meta property="og:image" content="http://localhost:1313/projects/CourseTA/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/projects/CourseTA/cover.png"><meta name=twitter:title content="CourseTA"><meta name=twitter:description content="An agent-based educational system using FastAPI, LangChain, and LangGraph in a microservice architecture. Implemented real-time async endpoints, integrated Whisper for audio/video transcription, and PyMuPDF for PDF parsing. Built a RAG-based QA pipeline with embedding-based retrieval, and AI agents for question generation, summarization, and feedback refinement. Optimized for scalable human-in-the-loop workflows and efficient content transformation across diverse formats."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"CourseTA","item":"http://localhost:1313/projects/courseta/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CourseTA","name":"CourseTA","description":"An agent-based educational system using FastAPI, LangChain, and LangGraph in a microservice architecture. Implemented real-time async endpoints, integrated Whisper for audio/video transcription, and PyMuPDF for PDF parsing. Built a RAG-based QA pipeline with embedding-based retrieval, and AI agents for question generation, summarization, and feedback refinement. Optimized for scalable human-in-the-loop workflows and efficient content transformation across diverse formats.","keywords":["Agentic-System","Langgraph","RAG","LLMs","Human-In-The-Loop","Async","Reflection-agent"],"articleBody":"\nCourseTA - AI Teaching Assistant CourseTA is an Agentic AI-powered teaching assistant that helps educators process educational content, generate questions, create summaries, and build Q\u0026A systems.\nFeatures File Upload: Upload PDF documents or audio/video files for automatic text extraction Question Generation: Create True/False or Multiple Choice questions from your content Content Summarization: Extract main points and generate comprehensive summaries Question Answering: Ask questions and get answers specific to your uploaded content Demo Requirements Python 3.9+ Dependencies listed in requirements.txt FFmpeg (for audio/video processing) Ollama (optional, for local LLM support) Installation Clone this repository:\n1 2 https://github.com/Sh-31/CourseTA.git cd CourseTA Install FFmpeg:\nLinux (Ubuntu/Debian):\n1 2 sudo apt update sudo apt install ffmpeg Install the required Python packages:\n1 pip install -r requirements.txt (Optional) Install Ollama for local LLM support:\nWindows/macOS/Linux:\nDownload and install from https://ollama.ai/ Or use the installation script: 1 curl -fsSL https://ollama.ai/install.sh | sh Pull the recommended model:\n1 ollama pull qwen3:4b Set up your environment variables (API keys, etc.) in a .env file.\nUpdate .env with your credentials:\n1 cp .env.example .env Usage Running the Application Start the FastAPI backend:\n1 python main.py In a separate terminal, start the Gradio UI:\n1 python gradio_ui.py Architecture CourseTA uses a microservice architecture with agent-based workflows:\nFastAPI backend for API endpoints LangChain-based processing pipelines with multi-agent workflows LangGraph for LLM orchestration Agent Graph Architecture CourseTA implements three main agent graphs, each designed with specific nodes, loops, and reflection mechanisms:\n1. Question Generation Graph The Question Generation agent follows a human-in-the-loop pattern with reflection capabilities:\nNodes:\nQuestionGenerator: Initial question creation from content HumanFeedback: Human interaction node with interrupt mechanism Router: Decision node that routes based on feedback type QuestionRefiner: Automatic refinement using AI feedback QuestionRewriter: Manual refinement based on human feedback Flow:\nStarts with question generation Enters human feedback loop with interrupt Router decides: save (END), auto (refiner), or feedback (rewriter) Both refiner and rewriter loop back to human feedback for continuous improvement 2. Content Summarization Graph The Summarization agent uses a two-stage approach with iterative refinement:\nNodes:\nSummarizerMainPointNode: Extracts key points and creates table of contents SummarizerWriterNode: Generates detailed summary from main points UserFeedbackNode: Human review and feedback collection SummarizerRewriterNode: Refines summary based on feedback Router: Routes to save or continue refinement Flow:\nSequential processing: Main Points → Summary Writer → User Feedback Feedback loop: Router directs to rewriter or completion Rewriter loops back to user feedback for iterative improvement 3. Question Answering Graph The Q\u0026A agent implements intelligent topic classification and retrieval:\nNodes:\nQuestionClassifier: Analyzes question relevance and retrieves context OnTopicRouter: Routes based on question relevance to content Retrieve: Fetches relevant document chunks using semantic search GenerateAnswer: Creates contextual answers from retrieved content OffTopicResponse: Handles questions outside the content scope Flow:\nQuestion classification with embedding-based relevance scoring Conditional routing: on-topic questions go through retrieval pipeline Off-topic questions receive appropriate redirect responses No loops - single-pass processing for efficiency Key Architectural Features Human-in-the-Loop Design:\nStrategic interrupt points for human feedback Continuous refinement loops in generation and summarization User control over when to complete or continue refining Reflection Agent Architecture:\nFeedback incorporation mechanisms History tracking for context preservation Iterative improvement through dedicated refiner/rewriter nodes Async API Architecture CourseTA implements a comprehensive async API architecture that supports both synchronous and streaming responses, providing real-time user experiences and efficient resource utilization.\nAPI Documentation File Processing APIs 1. Upload File Upload PDF documents or audio/video files for text extraction and processing.\nURL: /upload_file/\nMethod: POST\nContent-Type: multipart/form-data\nRequest Body:\n1 file: Upload file (PDF, audio, or video format) Response:\n1 2 3 4 5 6 { \"message\": \"File processed successfully\", \"id\": \"uuid-string\", \"text_path\": \"path/to/extracted_text.txt\", \"original_file_path\": \"path/to/original_file\" } Supported Formats:\nPDF: .pdf files Audio: .mp3, .wav formats Video: .mp4, .avi, .mov, .mkv, .flv formats 2. Get Extracted Text Retrieve the processed text content for a given asset ID.\nURL: /get_extracted_text/{asset_id}\nMethod: GET\nPath Parameters:\nasset_id: The unique identifier returned from file upload Response:\n1 2 3 4 { \"asset_id\": \"uuid-string\", \"extracted_text\": \"Full text content...\" } Question Generation APIs 3. Start Question Generation Session Generate questions from uploaded content with human-in-the-loop feedback.\nURL: /api/v1/graph/qg/start_session\nMethod: POST\nRequest Body:\n1 2 3 4 { \"asset_id\": \"uuid-string\", \"question_type\": \"T/F\" // or \"MCQ\" } Parameters:\nasset_id: Asset ID from file upload (required) question_type: Question type - “T/F” for True/False or “MCQ” for Multiple Choice (required) Response:\n1 2 3 4 5 6 7 8 9 10 11 12 { \"thread_id\": \"uuid-string\", \"status\": \"interrupted_for_feedback\", \"data_for_feedback\": { \"generated_question\": \"string\", \"options\": [\"string\"], // or null \"answer\": \"string\", \"explanation\": \"string\", \"message\": \"string\" }, \"current_state\": {} } 4. Provide Question Feedback Provide feedback to refine generated questions or save the current question.\nURL: /api/v1/graph/qg/provide_feedback\nMethod: POST\nRequest Body:\n1 2 3 4 { \"thread_id\": \"uuid-string\", \"feedback\": \"string\" } Parameters:\nthread_id: Session ID from start_session (required) feedback: Feedback text, “auto” for automatic refinement, or “save” to finish (required) Response:\n1 2 3 4 5 { \"thread_id\": \"uuid-string\", \"status\": \"completed\", // or \"interrupted_for_feedback\" \"final_state\": {} // or null } Content Summarization APIs 5. Start Summarization Session (Streaming) Generate content summaries with real-time streaming output.\nURL: /api/v1/graph/summarizer/start_session_streaming\nMethod: POST\nContent-Type: text/event-stream\nRequest Body:\n1 2 3 { \"asset_id\": \"uuid-string\" } Parameters:\nasset_id: Asset ID from file upload (required) Streaming Response Events:\n1 2 3 4 data: {\"thread_id\": \"uuid\", \"status\": \"starting_session\"} data: {\"event\": \"token\", \"token\": \"text\", \"status_update\": \"main_point_summarizer\"} data: {\"event\": \"token\", \"token\": \"text\", \"status_update\": \"summarizer_writer\"} data: {\"event\": \"stream_end\", \"thread_id\": \"uuid\", \"status_update\": \"Stream ended\"} 6. Provide Summarization Feedback (Streaming) Refine summaries based on user feedback with streaming responses.\nURL: /api/v1/graph/summarizer/provide_feedback_streaming\nMethod: POST\nContent-Type: text/event-stream\nRequest Body:\n1 2 3 4 { \"thread_id\": \"uuid-string\", \"feedback\": \"string\" } Parameters:\nthread_id: Session ID from start_session_streaming (required) feedback: Feedback text or “save” to finish (required) Streaming Response Events:\n1 2 3 data: {\"thread_id\": \"uuid\", \"status\": \"resuming_with_feedback\"} data: {\"event\": \"token\", \"token\": \"text\", \"status_update\": \"summarizer_rewriter\"} data: {\"event\": \"stream_end\", \"thread_id\": \"uuid\", \"status_update\": \"Stream ended\"} Question Answering APIs 7. Start Q\u0026A Session (Streaming) Answer questions based on uploaded content with streaming responses.\nURL: /api/v1/graph/qa/start_session_stream\nMethod: POST\nContent-Type: text/event-stream\nRequest Body:\n1 2 3 4 { \"asset_id\": \"uuid-string\", \"initial_question\": \"string\" } Parameters:\nasset_id: Asset ID from file upload (required) initial_question: The first question to ask about the content (required) Streaming Response Events:\n1 2 3 data: {\"type\": \"metadata\", \"thread_id\": \"uuid\", \"asset_id\": \"uuid\"} data: {\"type\": \"token\", \"content\": \"answer text...\"} data: {\"type\": \"complete\"} 8. Continue Q\u0026A Conversation (Streaming) Continue an existing Q\u0026A session with follow-up questions.\nURL: /api/v1/graph/qa/continue_conversation_stream\nMethod: POST\nContent-Type: text/event-stream\nRequest Body:\n1 2 3 4 { \"thread_id\": \"uuid-string\", \"next_question\": \"string\" } Streaming Response Events:\n1 2 3 data: {\"type\": \"metadata\", \"thread_id\": \"uuid\"} data: {\"type\": \"token\", \"content\": \"answer text...\"} data: {\"type\": \"complete\"} Headers for Streaming APIs Required Headers:\n1 2 3 Accept: text/event-stream Cache-Control: no-cache Connection: keep-alive ","wordCount":"1108","inLanguage":"en","image":"http://localhost:1313/projects/CourseTA/cover.png","datePublished":"2025-06-03T00:00:00Z","dateModified":"2025-06-03T00:00:00Z","author":{"@type":"Person","name":"Sherif Ahmed"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/courseta/"},"publisher":{"@type":"Organization","name":"Sherif Ahmed","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body class=dark id=top><header class=header style="position:sticky;top:0;z-index:100;background:var(--theme);box-shadow:0 2px 4px rgba(0,0,0,1%)"><nav class=nav><div class=logo><a href=http://localhost:1313/ class=header-terminal><span class=header-terminal-text></span>
<span class=header-terminal-cursor></span></a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/projects/ title="My Projects"><span>My Projects</span></a></li><li><a href=http://localhost:1313/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">CourseTA</h1><div class=post-description>An agent-based educational system using FastAPI, LangChain, and LangGraph in a microservice architecture. Implemented real-time async endpoints, integrated Whisper for audio/video transcription, and PyMuPDF for PDF parsing. Built a RAG-based QA pipeline with embedding-based retrieval, and AI agents for question generation, summarization, and feedback refinement. Optimized for scalable human-in-the-loop workflows and efficient content transformation across diverse formats.</div><div class=post-meta><span title='2025-06-03 00:00:00 +0000 UTC'>June 3, 3030</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1108 words&nbsp;·&nbsp;Sherif Ahmed</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/projects/CourseTA/cover.png alt=CourseTA></figure><div class=post-content><p><a href=https://github.com/Sh-31/CourseTA><img alt=Repo loading=lazy src="https://img.shields.io/badge/github-repo-black?logo=github&style=for-the-badge&scale=2"></a></p><h1 id=courseta---ai-teaching-assistant>CourseTA - AI Teaching Assistant<a hidden class=anchor aria-hidden=true href=#courseta---ai-teaching-assistant>#</a></h1><p>CourseTA is an Agentic AI-powered teaching assistant that helps educators process educational content, generate questions, create summaries, and build Q&amp;A systems.</p><h2 id=features>Features<a hidden class=anchor aria-hidden=true href=#features>#</a></h2><ul><li><strong>File Upload</strong>: Upload PDF documents or audio/video files for automatic text extraction</li><li><strong>Question Generation</strong>: Create True/False or Multiple Choice questions from your content</li><li><strong>Content Summarization</strong>: Extract main points and generate comprehensive summaries</li><li><strong>Question Answering</strong>: Ask questions and get answers specific to your uploaded content</li></ul><h2 id=demo>Demo<a hidden class=anchor aria-hidden=true href=#demo>#</a></h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/w-8KdO5Tlbc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><h2 id=requirements>Requirements<a hidden class=anchor aria-hidden=true href=#requirements>#</a></h2><ul><li>Python 3.9+</li><li>Dependencies listed in <code>requirements.txt</code></li><li>FFmpeg (for audio/video processing)</li><li>Ollama (optional, for local LLM support)</li></ul><h2 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h2><ol><li><p>Clone this repository:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>https://github.com/Sh-31/CourseTA.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> CourseTA
</span></span></code></pre></td></tr></table></div></div></li><li><p>Install FFmpeg:</p><p><strong>Linux (Ubuntu/Debian):</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo apt update
</span></span><span class=line><span class=cl>sudo apt install ffmpeg
</span></span></code></pre></td></tr></table></div></div></li><li><p>Install the required Python packages:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install -r requirements.txt
</span></span></code></pre></td></tr></table></div></div></li><li><p>(Optional) Install Ollama for local LLM support:</p><p><strong>Windows/macOS/Linux:</strong></p><ul><li>Download and install from <a href=https://ollama.ai/>https://ollama.ai/</a></li><li>Or use the installation script:</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl -fsSL https://ollama.ai/install.sh <span class=p>|</span> sh
</span></span></code></pre></td></tr></table></div></div><p><strong>Pull the recommended model:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>ollama pull qwen3:4b
</span></span></code></pre></td></tr></table></div></div></li><li><p>Set up your environment variables (API keys, etc.) in a <code>.env</code> file.</p><p><strong>Update <code>.env</code> with your credentials:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cp .env.example .env
</span></span></code></pre></td></tr></table></div></div></li></ol><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><h3 id=running-the-application>Running the Application<a hidden class=anchor aria-hidden=true href=#running-the-application>#</a></h3><ol><li><p>Start the FastAPI backend:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python main.py
</span></span></code></pre></td></tr></table></div></div></li><li><p>In a separate terminal, start the Gradio UI:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python gradio_ui.py
</span></span></code></pre></td></tr></table></div></div></li></ol><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>CourseTA uses a microservice architecture with agent-based workflows:</p><ul><li><strong>FastAPI backend</strong> for API endpoints</li><li><strong>LangChain-based processing pipelines</strong> with multi-agent workflows</li><li><strong>LangGraph</strong> for LLM orchestration</li></ul><h3 id=agent-graph-architecture>Agent Graph Architecture<a hidden class=anchor aria-hidden=true href=#agent-graph-architecture>#</a></h3><p>CourseTA implements three main agent graphs, each designed with specific nodes, loops, and reflection mechanisms:</p><h4 id=1-question-generation-graph>1. Question Generation Graph<a hidden class=anchor aria-hidden=true href=#1-question-generation-graph>#</a></h4><p><img alt="Question Generation Graph" loading=lazy src=/projects/CourseTA/docs/question_generation_graph.png></p><p>The Question Generation agent follows a human-in-the-loop pattern with reflection capabilities:</p><p><strong>Nodes:</strong></p><ul><li><strong>QuestionGenerator</strong>: Initial question creation from content</li><li><strong>HumanFeedback</strong>: Human interaction node with interrupt mechanism</li><li><strong>Router</strong>: Decision node that routes based on feedback type</li><li><strong>QuestionRefiner</strong>: Automatic refinement using AI feedback</li><li><strong>QuestionRewriter</strong>: Manual refinement based on human feedback</li></ul><p><strong>Flow:</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/nhtt__VREaI?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><ul><li>Starts with question generation</li><li>Enters human feedback loop with interrupt</li><li>Router decides: <code>save</code> (END), <code>auto</code> (refiner), or <code>feedback</code> (rewriter)</li><li>Both refiner and rewriter loop back to human feedback for continuous improvement</li></ul><h4 id=2-content-summarization-graph>2. Content Summarization Graph<a hidden class=anchor aria-hidden=true href=#2-content-summarization-graph>#</a></h4><p><img alt="Summarization Graph" loading=lazy src=/projects/CourseTA/docs/summarization_graph.png></p><p>The Summarization agent uses a two-stage approach with iterative refinement:</p><p><strong>Nodes:</strong></p><ul><li><strong>SummarizerMainPointNode</strong>: Extracts key points and creates table of contents</li><li><strong>SummarizerWriterNode</strong>: Generates detailed summary from main points</li><li><strong>UserFeedbackNode</strong>: Human review and feedback collection</li><li><strong>SummarizerRewriterNode</strong>: Refines summary based on feedback</li><li><strong>Router</strong>: Routes to save or continue refinement</li></ul><p><strong>Flow:</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/zJYlhPnnnbo?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><ul><li>Sequential processing: Main Points → Summary Writer → User Feedback</li><li>Feedback loop: Router directs to rewriter or completion</li><li>Rewriter loops back to user feedback for iterative improvement</li></ul><h4 id=3-question-answering-graph>3. Question Answering Graph<a hidden class=anchor aria-hidden=true href=#3-question-answering-graph>#</a></h4><p><img alt="Question Answer Graph" loading=lazy src=/projects/CourseTA/docs/question_answer_graph.png></p><p>The Q&amp;A agent implements intelligent topic classification and retrieval:</p><p><strong>Nodes:</strong></p><ul><li><strong>QuestionClassifier</strong>: Analyzes question relevance and retrieves context</li><li><strong>OnTopicRouter</strong>: Routes based on question relevance to content</li><li><strong>Retrieve</strong>: Fetches relevant document chunks using semantic search</li><li><strong>GenerateAnswer</strong>: Creates contextual answers from retrieved content</li><li><strong>OffTopicResponse</strong>: Handles questions outside the content scope</li></ul><p><strong>Flow:</strong></p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/ywezXaf_ebM?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="YouTube video"></iframe></div><ul><li>Question classification with embedding-based relevance scoring</li><li>Conditional routing: on-topic questions go through retrieval pipeline</li><li>Off-topic questions receive appropriate redirect responses</li><li>No loops - single-pass processing for efficiency</li></ul><h3 id=key-architectural-features>Key Architectural Features<a hidden class=anchor aria-hidden=true href=#key-architectural-features>#</a></h3><p><strong>Human-in-the-Loop Design:</strong></p><ul><li>Strategic interrupt points for human feedback</li><li>Continuous refinement loops in generation and summarization</li><li>User control over when to complete or continue refining</li></ul><p><strong>Reflection Agent Architecture:</strong></p><ul><li>Feedback incorporation mechanisms</li><li>History tracking for context preservation</li><li>Iterative improvement through dedicated refiner/rewriter nodes</li></ul><h3 id=async-api-architecture>Async API Architecture<a hidden class=anchor aria-hidden=true href=#async-api-architecture>#</a></h3><p>CourseTA implements a comprehensive async API architecture that supports both synchronous and streaming responses, providing real-time user experiences and efficient resource utilization.</p><h2 id=api-documentation>API Documentation<a hidden class=anchor aria-hidden=true href=#api-documentation>#</a></h2><h3 id=file-processing-apis>File Processing APIs<a hidden class=anchor aria-hidden=true href=#file-processing-apis>#</a></h3><h4 id=1-upload-file>1. Upload File<a hidden class=anchor aria-hidden=true href=#1-upload-file>#</a></h4><p>Upload PDF documents or audio/video files for text extraction and processing.</p><p><strong>URL:</strong> <code>/upload_file/</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Content-Type:</strong> <code>multipart/form-data</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-gdscript3 data-lang=gdscript3><span class=line><span class=cl><span class=n>file</span><span class=p>:</span> <span class=n>Upload</span> <span class=n>file</span> <span class=p>(</span><span class=n>PDF</span><span class=p>,</span> <span class=n>audio</span><span class=p>,</span> <span class=ow>or</span> <span class=n>video</span> <span class=n>format</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Response:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;File processed successfully&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;text_path&#34;</span><span class=p>:</span> <span class=s2>&#34;path/to/extracted_text.txt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;original_file_path&#34;</span><span class=p>:</span> <span class=s2>&#34;path/to/original_file&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Supported Formats:</strong></p><ul><li><strong>PDF</strong>: <code>.pdf</code> files</li><li><strong>Audio</strong>: <code>.mp3</code>, <code>.wav</code> formats</li><li><strong>Video</strong>: <code>.mp4</code>, <code>.avi</code>, <code>.mov</code>, <code>.mkv</code>, <code>.flv</code> formats</li></ul><hr><h4 id=2-get-extracted-text>2. Get Extracted Text<a hidden class=anchor aria-hidden=true href=#2-get-extracted-text>#</a></h4><p>Retrieve the processed text content for a given asset ID.</p><p><strong>URL:</strong> <code>/get_extracted_text/{asset_id}</code></p><p><strong>Method:</strong> <code>GET</code></p><p><strong>Path Parameters:</strong></p><ul><li><code>asset_id</code>: The unique identifier returned from file upload</li></ul><p><strong>Response:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;asset_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;extracted_text&#34;</span><span class=p>:</span> <span class=s2>&#34;Full text content...&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=question-generation-apis>Question Generation APIs<a hidden class=anchor aria-hidden=true href=#question-generation-apis>#</a></h3><h4 id=3-start-question-generation-session>3. Start Question Generation Session<a hidden class=anchor aria-hidden=true href=#3-start-question-generation-session>#</a></h4><p>Generate questions from uploaded content with human-in-the-loop feedback.</p><p><strong>URL:</strong> <code>/api/v1/graph/qg/start_session</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-jsonc data-lang=jsonc><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;asset_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;question_type&#34;</span><span class=p>:</span> <span class=s2>&#34;T/F&#34;</span>  <span class=c1>// or &#34;MCQ&#34;
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Parameters:</strong></p><ul><li><code>asset_id</code>: Asset ID from file upload (required)</li><li><code>question_type</code>: Question type - &ldquo;T/F&rdquo; for True/False or &ldquo;MCQ&rdquo; for Multiple Choice (required)</li></ul><p><strong>Response:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-jsonc data-lang=jsonc><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;thread_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;interrupted_for_feedback&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;data_for_feedback&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;generated_question&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;options&#34;</span><span class=p>:</span> <span class=p>[</span><span class=s2>&#34;string&#34;</span><span class=p>],</span>  <span class=c1>// or null
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=nt>&#34;answer&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;explanation&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;message&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;current_state&#34;</span><span class=p>:</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><hr><h4 id=4-provide-question-feedback>4. Provide Question Feedback<a hidden class=anchor aria-hidden=true href=#4-provide-question-feedback>#</a></h4><p>Provide feedback to refine generated questions or save the current question.</p><p><strong>URL:</strong> <code>/api/v1/graph/qg/provide_feedback</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;thread_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;feedback&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Parameters:</strong></p><ul><li><code>thread_id</code>: Session ID from start_session (required)</li><li><code>feedback</code>: Feedback text, &ldquo;auto&rdquo; for automatic refinement, or &ldquo;save&rdquo; to finish (required)</li></ul><p><strong>Response:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-jsonc data-lang=jsonc><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;thread_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;status&#34;</span><span class=p>:</span> <span class=s2>&#34;completed&#34;</span><span class=p>,</span> <span class=c1>// or &#34;interrupted_for_feedback&#34;
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nt>&#34;final_state&#34;</span><span class=p>:</span> <span class=p>{}</span>  <span class=c1>// or null
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=content-summarization-apis>Content Summarization APIs<a hidden class=anchor aria-hidden=true href=#content-summarization-apis>#</a></h3><h4 id=5-start-summarization-session-streaming>5. Start Summarization Session (Streaming)<a hidden class=anchor aria-hidden=true href=#5-start-summarization-session-streaming>#</a></h4><p>Generate content summaries with real-time streaming output.</p><p><strong>URL:</strong> <code>/api/v1/graph/summarizer/start_session_streaming</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Content-Type:</strong> <code>text/event-stream</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;asset_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Parameters:</strong></p><ul><li><code>asset_id</code>: Asset ID from file upload (required)</li></ul><p><strong>Streaming Response Events:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>data: {&#34;thread_id&#34;: &#34;uuid&#34;, &#34;status&#34;: &#34;starting_session&#34;}
</span></span><span class=line><span class=cl>data: {&#34;event&#34;: &#34;token&#34;, &#34;token&#34;: &#34;text&#34;, &#34;status_update&#34;: &#34;main_point_summarizer&#34;}
</span></span><span class=line><span class=cl>data: {&#34;event&#34;: &#34;token&#34;, &#34;token&#34;: &#34;text&#34;, &#34;status_update&#34;: &#34;summarizer_writer&#34;}
</span></span><span class=line><span class=cl>data: {&#34;event&#34;: &#34;stream_end&#34;, &#34;thread_id&#34;: &#34;uuid&#34;, &#34;status_update&#34;: &#34;Stream ended&#34;}
</span></span></code></pre></td></tr></table></div></div><hr><h4 id=6-provide-summarization-feedback-streaming>6. Provide Summarization Feedback (Streaming)<a hidden class=anchor aria-hidden=true href=#6-provide-summarization-feedback-streaming>#</a></h4><p>Refine summaries based on user feedback with streaming responses.</p><p><strong>URL:</strong> <code>/api/v1/graph/summarizer/provide_feedback_streaming</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Content-Type:</strong> <code>text/event-stream</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;thread_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;feedback&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Parameters:</strong></p><ul><li><code>thread_id</code>: Session ID from start_session_streaming (required)</li><li><code>feedback</code>: Feedback text or &ldquo;save&rdquo; to finish (required)</li></ul><p><strong>Streaming Response Events:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>data: {&#34;thread_id&#34;: &#34;uuid&#34;, &#34;status&#34;: &#34;resuming_with_feedback&#34;}
</span></span><span class=line><span class=cl>data: {&#34;event&#34;: &#34;token&#34;, &#34;token&#34;: &#34;text&#34;, &#34;status_update&#34;: &#34;summarizer_rewriter&#34;}
</span></span><span class=line><span class=cl>data: {&#34;event&#34;: &#34;stream_end&#34;, &#34;thread_id&#34;: &#34;uuid&#34;, &#34;status_update&#34;: &#34;Stream ended&#34;}
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=question-answering-apis>Question Answering APIs<a hidden class=anchor aria-hidden=true href=#question-answering-apis>#</a></h3><h4 id=7-start-qa-session-streaming>7. Start Q&amp;A Session (Streaming)<a hidden class=anchor aria-hidden=true href=#7-start-qa-session-streaming>#</a></h4><p>Answer questions based on uploaded content with streaming responses.</p><p><strong>URL:</strong> <code>/api/v1/graph/qa/start_session_stream</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Content-Type:</strong> <code>text/event-stream</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;asset_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;initial_question&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Parameters:</strong></p><ul><li><code>asset_id</code>: Asset ID from file upload (required)</li><li><code>initial_question</code>: The first question to ask about the content (required)</li></ul><p><strong>Streaming Response Events:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>data: {&#34;type&#34;: &#34;metadata&#34;, &#34;thread_id&#34;: &#34;uuid&#34;, &#34;asset_id&#34;: &#34;uuid&#34;}
</span></span><span class=line><span class=cl>data: {&#34;type&#34;: &#34;token&#34;, &#34;content&#34;: &#34;answer text...&#34;}
</span></span><span class=line><span class=cl>data: {&#34;type&#34;: &#34;complete&#34;}
</span></span></code></pre></td></tr></table></div></div><hr><h4 id=8-continue-qa-conversation-streaming>8. Continue Q&amp;A Conversation (Streaming)<a hidden class=anchor aria-hidden=true href=#8-continue-qa-conversation-streaming>#</a></h4><p>Continue an existing Q&amp;A session with follow-up questions.</p><p><strong>URL:</strong> <code>/api/v1/graph/qa/continue_conversation_stream</code></p><p><strong>Method:</strong> <code>POST</code></p><p><strong>Content-Type:</strong> <code>text/event-stream</code></p><p><strong>Request Body:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;thread_id&#34;</span><span class=p>:</span> <span class=s2>&#34;uuid-string&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;next_question&#34;</span><span class=p>:</span> <span class=s2>&#34;string&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Streaming Response Events:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>data: {&#34;type&#34;: &#34;metadata&#34;, &#34;thread_id&#34;: &#34;uuid&#34;}
</span></span><span class=line><span class=cl>data: {&#34;type&#34;: &#34;token&#34;, &#34;content&#34;: &#34;answer text...&#34;}
</span></span><span class=line><span class=cl>data: {&#34;type&#34;: &#34;complete&#34;}
</span></span></code></pre></td></tr></table></div></div><hr><h3 id=headers-for-streaming-apis>Headers for Streaming APIs<a hidden class=anchor aria-hidden=true href=#headers-for-streaming-apis>#</a></h3><p><strong>Required Headers:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Accept: text/event-stream
</span></span><span class=line><span class=cl>Cache-Control: no-cache
</span></span><span class=line><span class=cl>Connection: keep-alive
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/agentic-system/>Agentic-System</a></li><li><a href=http://localhost:1313/tags/langgraph/>Langgraph</a></li><li><a href=http://localhost:1313/tags/rag/>RAG</a></li><li><a href=http://localhost:1313/tags/llms/>LLMs</a></li><li><a href=http://localhost:1313/tags/human-in-the-loop/>Human-in-the-Loop</a></li><li><a href=http://localhost:1313/tags/async/>Async</a></li><li><a href=http://localhost:1313/tags/reflection-agent/>Reflection-Agent</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/projects/group-activity-recognition/><span class=title>Next »</span><br><span>Group Activity Recognition</span></a></nav></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><style>.terminal-line-container{display:flex;justify-content:center;margin-top:5rem;margin-bottom:2rem}.terminal-line{font-family:monospace;background-color:#212121;padding:15px;border-radius:5px;color:#fafafa;display:flex;align-items:center;min-width:300px}.terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:#fafafa;animation:blink 1s step-end infinite;margin-left:4px}@keyframes blink{from,to{background-color:transparent}50%{background-color:#fafafa}}.header-terminal{font-family:courier new,Courier,monospace;padding:6px 12px;color:rgba(255,255,255,.9);display:flex;align-items:center;font-size:.95rem;margin-right:16px;text-decoration:none;transition:all .2s ease;letter-spacing:-.5px}.header-terminal:hover{transform:translateY(-1px);opacity:.8}.header-terminal-text{white-space:nowrap;font-weight:600;color:rgba(255,255,255,.9)}.header-terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:rgba(255,255,255,.9);animation:blink .8s step-end infinite;margin-left:4px;opacity:.9}@media(max-width:600px){.header-terminal{font-size:.85rem;padding:4px 8px}}.footer-content{display:flex;flex-direction:row;align-items:center;justify-content:center;padding:0rem 1;gap:.5rem;text-align:center}.copyright{color:var(--secondary);font-size:.9rem}.author{color:var(--primary);font-size:1rem;font-weight:500}.entry-content .profile-mode-image img{border-radius:50%;object-fit:cover;aspect-ratio:1;border:3px solid var(--primary);transition:transform .3s ease,border-color .3s ease}.entry-content .profile-mode-image img:hover{transform:scale(1.05);border-color:var(--secondary)}</style><script>(function(){const t="cd /home/about",n=document.getElementById("terminal-text");if(!n)return;let e=0;function s(){if(e<t.length)n.textContent+=t.charAt(e),e++,setTimeout(s,120);else{const e=document.querySelector(".terminal-cursor");e&&(e.style.animation="none",e.style.backgroundColor="#fafafa")}}const o=new IntersectionObserver(e=>{e.forEach(e=>{e.isIntersecting&&(setTimeout(s,500),o.unobserve(e.target))})}),i=document.querySelector(".terminal-line");i&&o.observe(i)})();function updateTerminalPath(){const t=window.location.pathname;let e="> cd";t==="/"?e+=" /home":t.endsWith("/")?e+=" /home"+t.slice(0,-1):e+=" /home"+t;const n=document.querySelector(".header-terminal-text");if(n){let t=0;n.textContent="> ";function o(){t<e.length&&(n.textContent=e.substring(0,t+1),t++,setTimeout(o,35))}o()}const s=document.getElementById("terminal-text");if(s){s.textContent="";let t=0;function i(){t<e.length&&(s.textContent=e.substring(0,t+1),t++,setTimeout(i,120))}setTimeout(i,500)}}document.addEventListener("DOMContentLoaded",updateTerminalPath),window.addEventListener("popstate",updateTerminalPath)</script><footer class=footer><div class=footer-content><div class=copyright>© 2025</div><div class=author>Sherif Ahmed</div></div></footer></body></html>