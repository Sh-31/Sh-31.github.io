<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Relational Group Activity Recognition | Sherif Ahmed</title><meta name=keywords content="Paper-Implementation,ECCV 2018,Distributed-Training,Torch,GNN,CNN,LSTM,Graph Attention,Spatial-Temporal,Computer Vision"><meta name=description content="Implemented a Hierarchical Relational Network architecture for Group Activity Recognition and Retrieval, based on the ECCV 2018 paper. This implementation models inter-person relations and hierarchical temporal dynamics using Relational Layer (Graph Relational Layer). Ablation experiments were conducted to assess the impact of relation modeling, attention mechanisms (Graph Attention Operator), and hierarchical layers."><meta name=author content="Sherif Ahmed"><link rel=canonical href=http://localhost:1313/projects/rgar/><link crossorigin=anonymous href=/assets/css/stylesheet.43675f0167f994c072fb1a6bd61d8502d9877943dd538a474f7d64fa346ba64c.css integrity="sha256-Q2dfAWf5lMBy+xpr1h2FAtmHeUPdU4pHT31k+jRrpkw=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/rgar/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/rgar/"><meta property="og:site_name" content="Sherif Ahmed"><meta property="og:title" content="Relational Group Activity Recognition"><meta property="og:description" content="Implemented a Hierarchical Relational Network architecture for Group Activity Recognition and Retrieval, based on the ECCV 2018 paper. This implementation models inter-person relations and hierarchical temporal dynamics using Relational Layer (Graph Relational Layer). Ablation experiments were conducted to assess the impact of relation modeling, attention mechanisms (Graph Attention Operator), and hierarchical layers."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-08-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-27T00:00:00+00:00"><meta property="article:tag" content="Paper-Implementation"><meta property="article:tag" content="ECCV 2018"><meta property="article:tag" content="Distributed-Training"><meta property="article:tag" content="Torch"><meta property="article:tag" content="GNN"><meta property="article:tag" content="CNN"><meta property="og:image" content="http://localhost:1313/projects/Relational_Group_Activity_Recognition/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/projects/Relational_Group_Activity_Recognition/cover.png"><meta name=twitter:title content="Relational Group Activity Recognition"><meta name=twitter:description content="Implemented a Hierarchical Relational Network architecture for Group Activity Recognition and Retrieval, based on the ECCV 2018 paper. This implementation models inter-person relations and hierarchical temporal dynamics using Relational Layer (Graph Relational Layer). Ablation experiments were conducted to assess the impact of relation modeling, attention mechanisms (Graph Attention Operator), and hierarchical layers."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"Relational Group Activity Recognition","item":"http://localhost:1313/projects/rgar/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Relational Group Activity Recognition","name":"Relational Group Activity Recognition","description":"Implemented a Hierarchical Relational Network architecture for Group Activity Recognition and Retrieval, based on the ECCV 2018 paper. This implementation models inter-person relations and hierarchical temporal dynamics using Relational Layer (Graph Relational Layer). Ablation experiments were conducted to assess the impact of relation modeling, attention mechanisms (Graph Attention Operator), and hierarchical layers.","keywords":["Paper-Implementation","ECCV 2018","Distributed-Training","Torch","GNN","CNN","LSTM","Graph Attention","Spatial-Temporal","Computer Vision"],"articleBody":"\nRelational Group Activity Recognition Table of Contents Key Updates Introduction How the Relational Layer Works Usage Clone the Repository Install Dependencies Download Model Checkpoint Option 1: Use Python Code Option 2: Download Directly Dataset Overview Example Annotations Train-Test Split Dataset Statistics Group Activity Labels Player Action Labels Dataset Organization Dataset Download Instructions Ablation Study Baselines Single Frame Models Performance Comparison (Original Paper) My Scores (Accuracy) Temporal Models Performance Comparison (Original Paper) My Scores (Accuracy) Attention Models (New Baseline) My Scores (Accuracy) Confusion Matrix Key Updates ResNet-50 Backbone: Replaced VGG19 with ResNet-50 for stronger feature extraction. Ablation Studies: Comprehensive experiments to evaluate the contribution of each model component. Test-Time Augmentation (TTA): Implemented to improve robustness and reliability during inference. Graph Attention Operator: Implementation for an attention-based relational layer. Improved Performance: Achieves consistently higher accuracy across all baselines compared to the original paper. Modern Implementation: Fully implemented in PyTorch with support from PyTorch Geometric. Introduction Traditional pooling methods (max, average, or attention pooling) reduce dimensionality but often discard important spatial and relational details between people. The Hierarchical Relational Network (HRN) addresses this by introducing a relational layer that explicitly models interactions between individuals in a structured relationship graph.\nHow the Relational Layer Works Graph Construction\nEach person in a frame is represented as a node. People are ordered based on the top-left corner (x, y) of their bounding boxes (first by x, then by y if tied). Edges connect a person to their neighbors, forming cliques in the graph. Initial Person Features\nEach person’s initial representation comes from a CNN backbone (e.g., ResNet50):\n$$P_i^0 = \\text{CNN}(I_i)$$\nwhere $I_i$ is the cropped image around person $i$.\nRelational Update\nAt relational layer $\\ell$, person $i$’s updated representation is:\n$$P_i^\\ell = \\sum_{j \\in E_i^\\ell} F^\\ell(P_i^{\\ell-1} \\oplus P_j^{\\ell-1}; \\theta^\\ell)$$\n$E_i^\\ell$: neighbors of person $i$ in graph $G^\\ell$ $\\oplus$: concatenation operator $F^\\ell$: shared MLP for layer $\\ell$ (input size $2N_{\\ell-1}$, output size $N_\\ell$) This step computes pairwise relation vectors between $i$ and its neighbors, then aggregates them. Hierarchical Stacking\nMultiple relational layers are stacked, compressing person features while refining relational context. The architecture supports a variable number of people $K$ (robust to occlusions or false detections). Scene Representation\nThe final scene feature $S$ is obtained by pooling person features from the last relational layer:\n$$S = P_1^L \\ ▽ \\ P_2^L \\ ▽ \\dots \\ ▽ \\ P_K^L$$\nwhere $▽$ is a pooling operator (e.g., concatenation or element-wise max pooling).\nUsage 1. Clone the Repository 1 git clone https://github.com/Sh-31/Relational-Group-Activity-Recognition.git 2. Install the Required Dependencies 1 pip3 install -r requirements.txt 3. Download the Model Checkpoint This is a manual step that involves downloading the model checkpoint files.\nOption 1: Use Python Code Replace the modeling folder with the downloaded folder:\n1 2 3 4 5 6 import kagglehub # Download latest version path = kagglehub.model_download(\"sherif31/relational-group-activity-recognition/pyTorch/default\") print(\"Path to model files:\", path) Option 2: Download Directly Browse and download the specific checkpoint from Kaggle:\nRelational-Group-Activity-Recognition - PyTorch Checkpoint\nDataset Overview The dataset was created using publicly available YouTube volleyball videos. The authors annotated 4,830 frames from 55 videos, categorizing player actions into 9 labels and team activities into 8 labels.\nExample Annotations Figure: A frame labeled as “Left Spike,” with bounding boxes around each player, demonstrating team activity annotations. Train-Test Split Training Set: 3,493 frames Testing Set: 1,337 frames Dataset Statistics Group Activity Labels Group Activity Class Instances Right set 644 Right spike 623 Right pass 801 Right winpoint 295 Left winpoint 367 Left pass 826 Left spike 642 Left set 633 Player Action Labels Action Class Instances Waiting 3,601 Setting 1,332 Digging 2,333 Falling 1,241 Spiking 1,216 Blocking 2,458 Jumping 341 Moving 5,121 Standing 38,696 Dataset Organization Videos: 55, each assigned a unique ID (0–54). Train Videos: 1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54. Validation Videos: 0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51. Test Videos: 4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47. Dataset Download Instructions Enable Kaggle’s public API. Follow the guide here: Kaggle API Documentation. Use the provided shell script: 1 2 3 chmod 600 .kaggle/kaggle.json chmod +x script/script_download_volleball_dataset.sh .script/script_download_volleball_dataset.sh For further information about dataset, you can check out the paper author’s repository:\nlink\nAblation Study Baselines Single Frame Models: B1-NoRelations: In the first stage, Resnet50 is fine-tuned and a person is represented with 2048-d features. In the second stage, each person is connected to a shared dense layer of 128 features. The person representations (each of length 128 features) are then pooled and fed to a softmax layer for group activity classification.\nRCRG-1R-1C: Pretrained Resnet50 network is fine-tuned and a person is represented with 2048-d features, then a single relational layer (1R), all people in 1 clique (1C), so all-pairs relationships are learned.\nRCRG-1R-1C-!tuned: Same as previous variant, but Pretrained Resnet50 network without fine-tuning.\nRCRG-2R-11C: Close to the RCRG-1R-1C variant, but uses 2 relational layers (2R) of sizes 256 and 128. The graphs of these 2 layers are 1 clique (11C) of all people. This variant and the next ones explore stacking layers with different graph structures.\nRCRG-2R-21C: Same as the previous model, but the first layer has 2 cliques, one per team. The second layer is all-pairs relations (1C).\nRCRG-3R-421C: There relational layers (of sizes 512, 256, and 128) with clique sizes of the layers set to (4, 2, 1). The first layer has 4 cliques, with each team divided into 2 cliques.\nPerformance comparison Original Paper Baselines Score My Scores (Accuracy) Model Test Acc Test Acc TTA (4) Paper Test ACC B1-no-relations 89.07% 89.06% 85.1% RCRG-1R-1C 89.42% - 86.5% RCRG-1R-1C-untuned 80.86% - 75.4% RCRG-2R-11C 89.15% - 86.1% RCRG-2R-21C 89.49% - 87.2% RCRG-3R-421C 88.97% - 86.4% RCRG-2R-11C-conc 89.60% 89.71% 88.3% RCRG-2R-21C-conc 89.60% 89.60% 86.7% RCRG-3R-421C-conc 89.23% - 87.3% Notes:\n-conc postfix is used to indicate concatenation pooling instead of max-pooling. Used 4 transform augmentation at TTA. Temporal Models: RCRG-2R-11C-conc-temporal: Uses 2 relational layers (2R) of sizes 256 and 128. The graphs of these 2 layers are 1 clique (11C) of all people.\nRCRG-2R-21C: The first layer has 2 cliques, one per team. The second layer is all-pairs relations (1C).\nPerformance comparison Original Paper Baselines Score My Scores (Accuracy) Model Test Acc Test Acc TTA (3) Paper Test ACC B1-no-relations-temporal 88.93% 89.60% - RCRG-2R-11C-conc-V1 90.50% 90.73% 89.5% RCRG-2R-11C-conc-V2 91.55% 91.62% 89.5% RCRG-2R-11C-conc-V3 91.40% 91.77% 89.5% RCRG-2R-21C - - 89.4% Notes:\nTemporal: postfix is used to indicate model work with a sequence of frames, not a frame. -conc postfix is used to indicate concatenation pooling instead of max-pooling. The original paper did not clearly specify where the LSTM unit should be integrated into the model.\nTo explore this, I implemented three possible variants: V1: LSTM before the relational layer → allows the relational layer to learn richer spatio-temporal features. V2: LSTM after the relational layer → enhances the relational features with temporal modeling. V3: LSTMs both before and after the relational layer → combines the strengths of V1 and V2. I decided to train RCRG-2R-11C-conc only, since it achieved the best performance in both my implementation and the paper’s results. I implemented B1-no-relations-temporal to evaluate the impact of the relational layer (This model was not included in the original paper). Attention Models (new baseline): Uses 2 relational layers (2R). The graphs of these two layers are one clique (11C) of all players, but this time using a graph attentional operator instead of an MLP for the relational layers. My Scores (Accuracy) Model Test Acc Test Acc TTA (3) Paper Test ACC RCRG-2R-11C-conc-V1 91.77% 92.00% - RCRG-2R-11C-conc-V1-Attention Confusion Matrix ","wordCount":"1274","inLanguage":"en","image":"http://localhost:1313/projects/Relational_Group_Activity_Recognition/cover.png","datePublished":"2025-08-27T00:00:00Z","dateModified":"2025-08-27T00:00:00Z","author":{"@type":"Person","name":"Sherif Ahmed"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/rgar/"},"publisher":{"@type":"Organization","name":"Sherif Ahmed","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.ico"}}}</script></head><body class=dark id=top><!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Your Website Title</title><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(function(){})}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><header class=header style="position:sticky;top:0;z-index:100;background:var(--theme);box-shadow:0 2px 4px rgba(0,0,0,1%)"><nav class=nav><div class=logo><a href=http://localhost:1313/ class=header-terminal><span class=header-terminal-text></span>
<span class=header-terminal-cursor></span></a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1313/experience/ title=Experience><span>Experience</span></a></li><li><a href=http://localhost:1313/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header></body></html><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">Relational Group Activity Recognition</h1><div class=post-description>Implemented a Hierarchical Relational Network architecture for Group Activity Recognition and Retrieval, based on the ECCV 2018 paper. This implementation models inter-person relations and hierarchical temporal dynamics using Relational Layer (Graph Relational Layer). Ablation experiments were conducted to assess the impact of relation modeling, attention mechanisms (Graph Attention Operator), and hierarchical layers.</div><div class=post-meta><span title='2025-08-27 00:00:00 +0000 UTC'>August 27, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1274 words&nbsp;·&nbsp;Sherif Ahmed</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/projects/Relational_Group_Activity_Recognition/cover.png alt=RGAR></figure><div class=post-content><p><a href=https://github.com/Sh-31/Relational-Group-Activity-Recognition><img alt=Repo loading=lazy src="https://img.shields.io/badge/github-repo-black?logo=github&style=for-the-badge&scale=2"></a></p><p><a href=https://openaccess.thecvf.com/content_ECCV_2018/papers/Mostafa_Ibrahim_Hierarchical_Relational_Networks_ECCV_2018_paper.pdf><img alt=arXiv loading=lazy src=https://img.shields.io/badge/arXiv-1607.02643-b31b1b.svg></a></p><h1 id=relational-group-activity-recognition>Relational Group Activity Recognition<a hidden class=anchor aria-hidden=true href=#relational-group-activity-recognition>#</a></h1><h2 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ol><li><a href=#key-updates>Key Updates</a></li><li><a href=#introduction>Introduction</a><ul><li><a href=#how-the-relational-layer-works>How the Relational Layer Works</a></li></ul></li><li><a href=#usage>Usage</a><ul><li><a href=#1-clone-the-repository>Clone the Repository</a></li><li><a href=#2-install-the-required-dependencies>Install Dependencies</a></li><li><a href=#3-download-the-model-checkpoint>Download Model Checkpoint</a><ul><li><a href=#option-1-use-python-code>Option 1: Use Python Code</a></li><li><a href=#option-2-download-directly>Option 2: Download Directly</a></li></ul></li></ul></li><li><a href=#dataset-overview>Dataset Overview</a><ul><li><a href=#example-annotations>Example Annotations</a></li><li><a href=#train-test-split>Train-Test Split</a></li><li><a href=#dataset-statistics>Dataset Statistics</a><ul><li><a href=#group-activity-labels>Group Activity Labels</a></li><li><a href=#player-action-labels>Player Action Labels</a></li></ul></li><li><a href=#dataset-organization>Dataset Organization</a></li><li><a href=#dataset-download-instructions>Dataset Download Instructions</a></li></ul></li><li><a href=#ablation-study>Ablation Study</a><ul><li><a href=#baselines>Baselines</a><ul><li><a href=#single-frame-models>Single Frame Models</a><ul><li><a href=#performance-comparison>Performance Comparison (Original Paper)</a></li><li><a href=#my-scores-accuracy>My Scores (Accuracy)</a></li></ul></li><li><a href=#temporal-models>Temporal Models</a><ul><li><a href=#performance-comparison-1>Performance Comparison (Original Paper)</a></li><li><a href=#my-scores-accuracy-1>My Scores (Accuracy)</a></li></ul></li><li><a href=#attention-models-new-baseline>Attention Models (New Baseline)</a><ul><li><a href=#my-scores-accuracy-2>My Scores (Accuracy)</a></li><li><a href=#rcrg-2r-11c-conc-v1-attention-confusion-matrix>Confusion Matrix</a></li></ul></li></ul></li></ul></li></ol><h2 id=key-updates>Key Updates<a hidden class=anchor aria-hidden=true href=#key-updates>#</a></h2><ul><li><strong>ResNet-50 Backbone</strong>: Replaced VGG19 with ResNet-50 for stronger feature extraction.</li><li><strong>Ablation Studies</strong>: Comprehensive experiments to evaluate the contribution of each model component.</li><li><strong>Test-Time Augmentation (TTA)</strong>: Implemented to improve robustness and reliability during inference.</li><li><strong>Graph Attention Operator</strong>: Implementation for an attention-based relational layer.</li><li><strong>Improved Performance</strong>: Achieves consistently higher accuracy across all baselines compared to the original paper.</li><li><strong>Modern Implementation</strong>: Fully implemented in <strong>PyTorch</strong> with support from <strong>PyTorch Geometric</strong>.</li></ul><hr><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>Traditional pooling methods (max, average, or attention pooling) reduce dimensionality but often discard important <strong>spatial</strong> and <strong>relational</strong> details between people. The <strong>Hierarchical Relational Network (HRN)</strong> addresses this by introducing a <strong>relational layer</strong> that explicitly models interactions between individuals in a <strong>structured relationship graph</strong>.</p><p align=center><img width=512 height=512 src=https://github.com/user-attachments/assets/639cb140-a4df-4cd4-befc-2e965030723c alt="Relational Layer Illustration"></p><h2 id=how-the-relational-layer-works>How the Relational Layer Works<a hidden class=anchor aria-hidden=true href=#how-the-relational-layer-works>#</a></h2><ol><li><p><strong>Graph Construction</strong></p><ul><li>Each person in a frame is represented as a node.</li><li>People are ordered based on the top-left corner <code>(x, y)</code> of their bounding boxes (first by x, then by y if tied).</li><li>Edges connect a person to their neighbors, forming <strong>cliques</strong> in the graph.</li></ul></li><li><p><strong>Initial Person Features</strong><br>Each person’s initial representation comes from a CNN backbone (e.g., ResNet50):</p><p>$$P_i^0 = \text{CNN}(I_i)$$</p><p>where $I_i$ is the cropped image around person $i$.</p></li><li><p><strong>Relational Update</strong></p></li></ol><p align=center><img src=https://github.com/user-attachments/assets/d965ea0e-8599-4d0a-b20e-73c50fbfe6d0 alt="Graph Structure Illustration" width=750></p><p>At relational layer $\ell$, person $i$’s updated representation is:</p><p>$$P_i^\ell = \sum_{j \in E_i^\ell} F^\ell(P_i^{\ell-1} \oplus P_j^{\ell-1}; \theta^\ell)$$</p><ul><li>$E_i^\ell$: neighbors of person $i$ in graph $G^\ell$</li><li>$\oplus$: concatenation operator</li><li>$F^\ell$: shared MLP for layer $\ell$ (input size $2N_{\ell-1}$, output size $N_\ell$)</li></ul><ul><li>This step computes pairwise relation vectors between $i$ and its neighbors, then aggregates them.</li></ul><ol start=3><li><p><strong>Hierarchical Stacking</strong></p><p align=center><img src=https://github.com/user-attachments/assets/8d8f4ea7-803c-486d-8fb7-00638445ddb7 alt="Graph Structure Illustration" width=750></p><ul><li>Multiple relational layers are stacked, compressing person features while refining relational context.</li><li>The architecture supports a variable number of people $K$ (robust to occlusions or false detections).</li></ul></li><li><p><strong>Scene Representation</strong><br>The final scene feature $S$ is obtained by pooling person features from the last relational layer:</p><p>$$S = P_1^L \ ▽ \ P_2^L \ ▽ \dots \ ▽ \ P_K^L$$</p><p>where $▽$ is a pooling operator (e.g., concatenation or element-wise max pooling).</p></li></ol><hr><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><hr><h3 id=1-clone-the-repository>1. Clone the Repository<a hidden class=anchor aria-hidden=true href=#1-clone-the-repository>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/Sh-31/Relational-Group-Activity-Recognition.git
</span></span></code></pre></td></tr></table></div></div><h3 id=2-install-the-required-dependencies>2. Install the Required Dependencies<a hidden class=anchor aria-hidden=true href=#2-install-the-required-dependencies>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip3 install -r requirements.txt
</span></span></code></pre></td></tr></table></div></div><h3 id=3-download-the-model-checkpoint>3. Download the Model Checkpoint<a hidden class=anchor aria-hidden=true href=#3-download-the-model-checkpoint>#</a></h3><p>This is a manual step that involves downloading the model checkpoint files.</p><h4 id=option-1-use-python-code>Option 1: Use Python Code<a hidden class=anchor aria-hidden=true href=#option-1-use-python-code>#</a></h4><p>Replace the <code>modeling</code> folder with the downloaded folder:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>kagglehub</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Download latest version</span>
</span></span><span class=line><span class=cl><span class=n>path</span> <span class=o>=</span> <span class=n>kagglehub</span><span class=o>.</span><span class=n>model_download</span><span class=p>(</span><span class=s2>&#34;sherif31/relational-group-activity-recognition/pyTorch/default&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Path to model files:&#34;</span><span class=p>,</span> <span class=n>path</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=option-2-download-directly>Option 2: Download Directly<a hidden class=anchor aria-hidden=true href=#option-2-download-directly>#</a></h4><p>Browse and download the specific checkpoint from Kaggle:<br><a href=https://www.kaggle.com/models/sherif31/relational-group-activity-recognition/>Relational-Group-Activity-Recognition - PyTorch Checkpoint</a></p><hr><h2 id=dataset-overview>Dataset Overview<a hidden class=anchor aria-hidden=true href=#dataset-overview>#</a></h2><p>The dataset was created using publicly available YouTube volleyball videos. The authors annotated 4,830 frames from 55 videos, categorizing player actions into 9 labels and team activities into 8 labels.</p><h3 id=example-annotations>Example Annotations<a hidden class=anchor aria-hidden=true href=#example-annotations>#</a></h3><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/50f906ad-c68c-4882-b9cf-9200f5a380c7></p><ul><li><strong>Figure</strong>: A frame labeled as &ldquo;Left Spike,&rdquo; with bounding boxes around each player, demonstrating team activity annotations.</li></ul><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/cca9447a-8b40-4330-a11d-dbc0feb230ff></p><h3 id=train-test-split>Train-Test Split<a hidden class=anchor aria-hidden=true href=#train-test-split>#</a></h3><ul><li><strong>Training Set</strong>: 3,493 frames</li><li><strong>Testing Set</strong>: 1,337 frames</li></ul><h3 id=dataset-statistics>Dataset Statistics<a hidden class=anchor aria-hidden=true href=#dataset-statistics>#</a></h3><h4 id=group-activity-labels>Group Activity Labels<a hidden class=anchor aria-hidden=true href=#group-activity-labels>#</a></h4><table><thead><tr><th>Group Activity Class</th><th>Instances</th></tr></thead><tbody><tr><td>Right set</td><td>644</td></tr><tr><td>Right spike</td><td>623</td></tr><tr><td>Right pass</td><td>801</td></tr><tr><td>Right winpoint</td><td>295</td></tr><tr><td>Left winpoint</td><td>367</td></tr><tr><td>Left pass</td><td>826</td></tr><tr><td>Left spike</td><td>642</td></tr><tr><td>Left set</td><td>633</td></tr></tbody></table><h4 id=player-action-labels>Player Action Labels<a hidden class=anchor aria-hidden=true href=#player-action-labels>#</a></h4><table><thead><tr><th>Action Class</th><th>Instances</th></tr></thead><tbody><tr><td>Waiting</td><td>3,601</td></tr><tr><td>Setting</td><td>1,332</td></tr><tr><td>Digging</td><td>2,333</td></tr><tr><td>Falling</td><td>1,241</td></tr><tr><td>Spiking</td><td>1,216</td></tr><tr><td>Blocking</td><td>2,458</td></tr><tr><td>Jumping</td><td>341</td></tr><tr><td>Moving</td><td>5,121</td></tr><tr><td>Standing</td><td>38,696</td></tr></tbody></table><h3 id=dataset-organization>Dataset Organization<a hidden class=anchor aria-hidden=true href=#dataset-organization>#</a></h3><ul><li><strong>Videos</strong>: 55, each assigned a unique ID (0–54).</li><li><strong>Train Videos</strong>: 1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54.</li><li><strong>Validation Videos</strong>: 0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51.</li><li><strong>Test Videos</strong>: 4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47.</li></ul><h3 id=dataset-download-instructions>Dataset Download Instructions<a hidden class=anchor aria-hidden=true href=#dataset-download-instructions>#</a></h3><ol><li>Enable Kaggle&rsquo;s public API. Follow the guide here: <a href=https://www.kaggle.com/docs/api>Kaggle API Documentation</a>.</li><li>Use the provided shell script:</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>  chmod <span class=m>600</span> .kaggle/kaggle.json 
</span></span><span class=line><span class=cl>  chmod +x script/script_download_volleball_dataset.sh
</span></span><span class=line><span class=cl>  .script/script_download_volleball_dataset.sh
</span></span></code></pre></td></tr></table></div></div><p>For further information about dataset, you can check out the paper author&rsquo;s repository:<br><a href=https://github.com/mostafa-saad/deep-activity-rec>link</a></p><hr><h2 id=ablation-study><a href="https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)#:~:text=In%20artificial%20intelligence%20(AI)%2C,resultant%20performance%20of%20the%20system">Ablation Study</a><a hidden class=anchor aria-hidden=true href=#ablation-study>#</a></h2><h3 id=baselines>Baselines<a hidden class=anchor aria-hidden=true href=#baselines>#</a></h3><h4 id=single-frame-models>Single Frame Models:<a hidden class=anchor aria-hidden=true href=#single-frame-models>#</a></h4><ul><li><p><strong>B1-NoRelations:</strong> In the first stage, Resnet50 is fine-tuned and a person is represented with 2048-d features. In the second stage, each person is connected to a shared dense layer of 128 features. The person representations (each of length 128 features) are then pooled and fed to a softmax layer for group activity classification.</p></li><li><p><strong>RCRG-1R-1C:</strong> Pretrained Resnet50 network is fine-tuned and a person is represented with 2048-d features, then a single relational layer (1R), all people in 1 clique (1C), so all-pairs relationships are learned.</p></li><li><p><strong>RCRG-1R-1C-!tuned:</strong> Same as previous variant, but Pretrained Resnet50 network without fine-tuning.</p></li><li><p><strong>RCRG-2R-11C:</strong> Close to the RCRG-1R-1C variant, but uses 2 relational layers (2R) of sizes 256 and 128. The graphs of these 2 layers are 1 clique (11C) of all people. This variant and the next ones explore stacking layers with different graph structures.</p></li><li><p><strong>RCRG-2R-21C:</strong> Same as the previous model, but the first layer has 2 cliques, one per team. The second layer is all-pairs relations (1C).</p></li><li><p><strong>RCRG-3R-421C:</strong> There relational layers (of sizes 512, 256, and 128) with clique sizes of the layers set to (4, 2, 1). The first layer has 4 cliques, with each team divided into 2 cliques.</p></li></ul><h5 id=performance-comparison>Performance comparison<a hidden class=anchor aria-hidden=true href=#performance-comparison>#</a></h5><h6 id=original-paper-baselines-score>Original Paper Baselines Score<a hidden class=anchor aria-hidden=true href=#original-paper-baselines-score>#</a></h6><img width=615 height=542 alt={98F2C621-4E89-47FD-A112-A25946D611F3} src=https://github.com/user-attachments/assets/4f7cc2f7-2b6d-472d-9555-d4d9b2de65cc><h6 id=my-scores-accuracy>My Scores (Accuracy)<a hidden class=anchor aria-hidden=true href=#my-scores-accuracy>#</a></h6><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Test Acc</th><th style=text-align:center>Test Acc TTA (4)</th><th style=text-align:center>Paper Test ACC</th></tr></thead><tbody><tr><td style=text-align:left>B1-no-relations</td><td style=text-align:center>89.07%</td><td style=text-align:center>89.06%</td><td style=text-align:center>85.1%</td></tr><tr><td style=text-align:left>RCRG-1R-1C</td><td style=text-align:center>89.42%</td><td style=text-align:center>-</td><td style=text-align:center>86.5%</td></tr><tr><td style=text-align:left>RCRG-1R-1C-untuned</td><td style=text-align:center>80.86%</td><td style=text-align:center>-</td><td style=text-align:center>75.4%</td></tr><tr><td style=text-align:left>RCRG-2R-11C</td><td style=text-align:center>89.15%</td><td style=text-align:center>-</td><td style=text-align:center>86.1%</td></tr><tr><td style=text-align:left>RCRG-2R-21C</td><td style=text-align:center>89.49%</td><td style=text-align:center>-</td><td style=text-align:center>87.2%</td></tr><tr><td style=text-align:left>RCRG-3R-421C</td><td style=text-align:center>88.97%</td><td style=text-align:center>-</td><td style=text-align:center>86.4%</td></tr><tr><td style=text-align:left><strong>RCRG-2R-11C-conc</strong></td><td style=text-align:center><strong>89.60%</strong></td><td style=text-align:center><strong>89.71%</strong></td><td style=text-align:center>88.3%</td></tr><tr><td style=text-align:left><strong>RCRG-2R-21C-conc</strong></td><td style=text-align:center><strong>89.60%</strong></td><td style=text-align:center>89.60%</td><td style=text-align:center>86.7%</td></tr><tr><td style=text-align:left>RCRG-3R-421C-conc</td><td style=text-align:center>89.23%</td><td style=text-align:center>-</td><td style=text-align:center>87.3%</td></tr></tbody></table><p>Notes:</p><ul><li><code>-conc</code> postfix is used to indicate concatenation pooling instead of max-pooling.</li><li>Used 4 transform augmentation at TTA.</li></ul><h4 id=temporal-models>Temporal Models:<a hidden class=anchor aria-hidden=true href=#temporal-models>#</a></h4><ul><li><p><strong>RCRG-2R-11C-conc-temporal:</strong> Uses 2 relational layers (2R) of sizes 256 and 128. The graphs of these 2 layers are 1 clique (11C) of all people.</p></li><li><p><strong>RCRG-2R-21C:</strong> The first layer has 2 cliques, one per team. The second layer is all-pairs relations (1C).</p></li></ul><h5 id=performance-comparison-1>Performance comparison<a hidden class=anchor aria-hidden=true href=#performance-comparison-1>#</a></h5><h6 id=original-paper-baselines-score-1>Original Paper Baselines Score<a hidden class=anchor aria-hidden=true href=#original-paper-baselines-score-1>#</a></h6><img width=523 height=323 alt={848262DC-9865-4F49-A7CA-60B08675A6B8} src=https://github.com/user-attachments/assets/de4eb7cb-3f3c-4320-baef-c7679055a6dd><h6 id=my-scores-accuracy-1>My Scores (Accuracy)<a hidden class=anchor aria-hidden=true href=#my-scores-accuracy-1>#</a></h6><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Test Acc</th><th style=text-align:center>Test Acc TTA (3)</th><th style=text-align:center>Paper Test ACC</th></tr></thead><tbody><tr><td style=text-align:left>B1-no-relations-temporal</td><td style=text-align:center>88.93%</td><td style=text-align:center>89.60%</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>RCRG-2R-11C-conc-V1</td><td style=text-align:center>90.50%</td><td style=text-align:center>90.73%</td><td style=text-align:center>89.5%</td></tr><tr><td style=text-align:left>RCRG-2R-11C-conc-V2</td><td style=text-align:center><strong>91.55%</strong></td><td style=text-align:center>91.62%</td><td style=text-align:center>89.5%</td></tr><tr><td style=text-align:left>RCRG-2R-11C-conc-V3</td><td style=text-align:center>91.40%</td><td style=text-align:center><strong>91.77%</strong></td><td style=text-align:center>89.5%</td></tr><tr><td style=text-align:left>RCRG-2R-21C</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>89.4%</td></tr></tbody></table><p>Notes:</p><ul><li><code>Temporal</code>: postfix is used to indicate model work with a sequence of frames, not a frame.</li><li><code>-conc</code> postfix is used to indicate concatenation pooling instead of max-pooling.</li><li>The original paper did not clearly specify where the LSTM unit should be integrated into the model.<br>To explore this, I implemented three possible variants:<ul><li><code>V1</code>: LSTM <strong>before</strong> the relational layer → allows the relational layer to learn richer spatio-temporal features.</li><li><code>V2</code>: LSTM <strong>after</strong> the relational layer → enhances the relational features with temporal modeling.</li><li><code>V3</code>: LSTMs <strong>both before and after</strong> the relational layer → combines the strengths of V1 and V2.</li></ul></li><li>I decided to train <code>RCRG-2R-11C-conc</code> only, since it achieved the best performance in both my implementation and the paper’s results.</li><li>I implemented <code>B1-no-relations-temporal</code> to evaluate the impact of the relational layer (This model was not included in the original paper).</li></ul><h4 id=attention-models-new-baseline>Attention Models (new baseline):<a hidden class=anchor aria-hidden=true href=#attention-models-new-baseline>#</a></h4><ul><li>Uses 2 relational layers (2R). The graphs of these two layers are one clique (11C) of all players, but this time using a graph attentional operator instead of an MLP for the relational layers.</li></ul><h6 id=my-scores-accuracy-2>My Scores (Accuracy)<a hidden class=anchor aria-hidden=true href=#my-scores-accuracy-2>#</a></h6><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Test Acc</th><th style=text-align:center>Test Acc TTA (3)</th><th style=text-align:center>Paper Test ACC</th></tr></thead><tbody><tr><td style=text-align:left>RCRG-2R-11C-conc-V1</td><td style=text-align:center>91.77%</td><td style=text-align:center><strong>92.00%</strong></td><td style=text-align:center>-</td></tr></tbody></table><h6 id=rcrg-2r-11c-conc-v1-attention-confusion-matrix>RCRG-2R-11C-conc-V1-Attention Confusion Matrix<a hidden class=anchor aria-hidden=true href=#rcrg-2r-11c-conc-v1-attention-confusion-matrix>#</a></h6><img src=https://raw.githubusercontent.com/Sh-31/Relational-Group-Activity-Recognition/refs/heads/main/experiments/attention_models/RCRG-R2-C11-conc-temporal_V1_2025_06_28_18_37/Group_Activity_RCRG-R2-C11-Conc-Temporal-Attention-TTA_Eval_On_Testset_TTA_confusion_matrix.png alt=RCRG-2R-11C-conc-V1></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/paper-implementation/>Paper-Implementation</a></li><li><a href=http://localhost:1313/tags/eccv-2018/>ECCV 2018</a></li><li><a href=http://localhost:1313/tags/distributed-training/>Distributed-Training</a></li><li><a href=http://localhost:1313/tags/torch/>Torch</a></li><li><a href=http://localhost:1313/tags/gnn/>GNN</a></li><li><a href=http://localhost:1313/tags/cnn/>CNN</a></li><li><a href=http://localhost:1313/tags/lstm/>LSTM</a></li><li><a href=http://localhost:1313/tags/graph-attention/>Graph Attention</a></li><li><a href=http://localhost:1313/tags/spatial-temporal/>Spatial-Temporal</a></li><li><a href=http://localhost:1313/tags/computer-vision/>Computer Vision</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/projects/group-activity-recognition/><span class=title>« Prev</span><br><span>Group Activity Recognition</span>
</a><a class=next href=http://localhost:1313/projects/imacap/><span class=title>Next »</span><br><span>ImgCap</span></a></nav></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=http://localhost:1313/js/experience-timeline.js></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><style>.terminal-line-container{display:flex;justify-content:center;margin-top:5rem;margin-bottom:2rem}.terminal-line{font-family:monospace;background-color:#212121;padding:15px;border-radius:5px;color:#fafafa;display:flex;align-items:center;min-width:300px}.terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:#fafafa;animation:blink 1s step-end infinite;margin-left:4px}@keyframes blink{from,to{background-color:initial}50%{background-color:#fafafa}}.header-terminal{font-family:courier new,Courier,monospace;padding:6px 12px;color:rgba(255,255,255,.9);display:flex;align-items:center;font-size:.95rem;margin-right:16px;text-decoration:none;transition:all .2s ease;letter-spacing:-.5px}.header-terminal:hover{transform:translateY(-1px);opacity:.8}.header-terminal-text{white-space:nowrap;font-weight:600;color:rgba(255,255,255,.9)}.header-terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:rgba(255,255,255,.9);animation:blink .8s step-end infinite;margin-left:4px;opacity:.9}@media(max-width:600px){.header-terminal{font-size:.85rem;padding:4px 8px}}.footer-content{display:flex;flex-direction:row;align-items:center;justify-content:center;padding:0rem 1;gap:.5rem;text-align:center}.copyright{color:var(--secondary);font-size:.9rem}.author{color:var(--primary);font-size:1rem;font-weight:500}.entry-content .profile-mode-image img{border-radius:50%;object-fit:cover;aspect-ratio:1;border:3px solid var(--primary);transition:transform .3s ease,border-color .3s ease}.entry-content .profile-mode-image img:hover{transform:scale(1.05);border-color:var(--secondary)}</style><script>(function(){const t="cd /home/about",n=document.getElementById("terminal-text");if(!n)return;let e=0;function s(){if(e<t.length)n.textContent+=t.charAt(e),e++,setTimeout(s,120);else{const e=document.querySelector(".terminal-cursor");e&&(e.style.animation="none",e.style.backgroundColor="#fafafa")}}const o=new IntersectionObserver(e=>{e.forEach(e=>{e.isIntersecting&&(setTimeout(s,500),o.unobserve(e.target))})}),i=document.querySelector(".terminal-line");i&&o.observe(i)})();function updateTerminalPath(){const t=window.location.pathname;let e="> cd";t==="/"?e+=" /home":t.endsWith("/")?e+=" /home"+t.slice(0,-1):e+=" /home"+t;const n=document.querySelector(".header-terminal-text");if(n){let t=0;n.textContent="> ";function o(){t<e.length&&(n.textContent=e.substring(0,t+1),t++,setTimeout(o,35))}o()}const s=document.getElementById("terminal-text");if(s){s.textContent="";let t=0;function i(){t<e.length&&(s.textContent=e.substring(0,t+1),t++,setTimeout(i,120))}setTimeout(i,500)}}document.addEventListener("DOMContentLoaded",updateTerminalPath),window.addEventListener("popstate",updateTerminalPath)</script><footer class=footer><div class=footer-content><div class=copyright>© 2025</div><div class=author>Sherif Ahmed</div></div></footer></body></html>