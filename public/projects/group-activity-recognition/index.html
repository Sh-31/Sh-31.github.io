<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Group Activity Recognition | Sherif Ahmed</title><meta name=keywords content="Paper-Implementation,CVPR-2016,Distributed-Training,Torch,Two-Stage Arch,Spatial-Temporal,Multi-Person-Recognition"><meta name=description content="A modern implementation implemented a Hierarchical Deep Temporal Model for Group Activity Recognition, based on the CVPR 2016 paper. Achieved 93% accuracy using a two-stage LSTM architecture to recognize multi-person activities. Conducted ablation studies to evaluate the contributions of various model components and compared performance against 8 baseline models."><meta name=author content="Sherif Ahmed"><link rel=canonical href=http://localhost:1313/projects/group-activity-recognition/><link crossorigin=anonymous href=/assets/css/stylesheet.023a79c09a6253ba6d2a259b4c3db25df8cce828b8b0e2cc06c6675372eb7b43.css integrity="sha256-Ajp5wJpiU7ptKiWbTD2yXfjM6Ci4sOLMBsZnU3Lre0M=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/projects/group-activity-recognition/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:1313/projects/group-activity-recognition/"><meta property="og:site_name" content="Sherif Ahmed"><meta property="og:title" content="Group Activity Recognition"><meta property="og:description" content="A modern implementation implemented a Hierarchical Deep Temporal Model for Group Activity Recognition, based on the CVPR 2016 paper. Achieved 93% accuracy using a two-stage LSTM architecture to recognize multi-person activities. Conducted ablation studies to evaluate the contributions of various model components and compared performance against 8 baseline models."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-02-24T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-24T00:00:00+00:00"><meta property="article:tag" content="Paper-Implementation"><meta property="article:tag" content="CVPR-2016"><meta property="article:tag" content="Distributed-Training"><meta property="article:tag" content="Torch"><meta property="article:tag" content="Two-Stage Arch"><meta property="article:tag" content="Spatial-Temporal"><meta property="og:image" content="http://localhost:1313/projects/Group_Activity_Recognition/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/projects/Group_Activity_Recognition/cover.png"><meta name=twitter:title content="Group Activity Recognition"><meta name=twitter:description content="A modern implementation implemented a Hierarchical Deep Temporal Model for Group Activity Recognition, based on the CVPR 2016 paper. Achieved 93% accuracy using a two-stage LSTM architecture to recognize multi-person activities. Conducted ablation studies to evaluate the contributions of various model components and compared performance against 8 baseline models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"http://localhost:1313/projects/"},{"@type":"ListItem","position":2,"name":"Group Activity Recognition","item":"http://localhost:1313/projects/group-activity-recognition/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Group Activity Recognition","name":"Group Activity Recognition","description":"A modern implementation implemented a Hierarchical Deep Temporal Model for Group Activity Recognition, based on the CVPR 2016 paper. Achieved 93% accuracy using a two-stage LSTM architecture to recognize multi-person activities. Conducted ablation studies to evaluate the contributions of various model components and compared performance against 8 baseline models.","keywords":["Paper-Implementation","CVPR-2016","Distributed-Training","Torch","Two-Stage Arch","Spatial-Temporal","Multi-Person-Recognition"],"articleBody":"\nGroup Activity Recognition Table of Contents Key Updates Usage Clone the Repository Install Dependencies Download Model Checkpoint Dataset Overview Example Annotations Train-Test Split Dataset Statistics Dataset Organization Dataset Download Instructions Ablation Study Baselines Performance Comparison Original Paper Baselines Score My Scores (Accuracy and F1 Scores) Interesting Observations Effect of Team Independent Pooling Model Architecture Key Updates ResNet50 for feature extraction (replacing AlexNet). Ablation studies to analyze model components. Implementation of an end-to-end version (Baseline 9). Achieve higher performance across every model baseline compared to the original paper. Full implementation in Python (original used Caffe). Usage 1. Clone the Repository 1 git clone https://github.com/Sh-31/Group-Activity-Recognition.git 2. Install the Required Dependencies 1 pip3 install -r requirements.txt 3. Download the Model Checkpoint This is a manual step that involves downloading the model checkpoint files.\nOption 1: Use Python Code Replace the modeling folder with the downloaded folder:\n1 2 3 4 5 6 import kagglehub # Download the latest version path = kagglehub.model_download(\"sherif31/group-activity-recognition/pyTorch/v1\") print(\"Path to model files:\", path) Option 2: Download Directly Browse and download the specific checkpoint from Kaggle:\nGroup Activity Recognition - PyTorch Checkpoint\nDataset Overview The dataset was created using publicly available YouTube volleyball videos. The authors annotated 4,830 frames from 55 videos, categorizing player actions into 9 labels and team activities into 8 labels.\nExample Annotations Figure: A frame labeled as “Left Spike,” with bounding boxes around each player, demonstrating team activity annotations. Train-Test Split Training Set: 3,493 frames Testing Set: 1,337 frames Dataset Statistics Group Activity Labels Group Activity Class Instances Right set 644 Right spike 623 Right pass 801 Right winpoint 295 Left winpoint 367 Left pass 826 Left spike 642 Left set 633 Player Action Labels Action Class Instances Waiting 3,601 Setting 1,332 Digging 2,333 Falling 1,241 Spiking 1,216 Blocking 2,458 Jumping 341 Moving 5,121 Standing 38,696 Dataset Organization Videos: 55, each assigned a unique ID (0–54). Train Videos: 1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54. Validation Videos: 0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51. Test Videos: 4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47. Dataset Download Instructions Enable Kaggle’s public API. Follow the guide here: Kaggle API Documentation. Use the provided shell script: 1 2 3 chmod 600 .kaggle/kaggle.json chmod +x script/script_download_volleball_dataset.sh .script/script_download_volleball_dataset.sh For further information about dataset, you can check out the paper author’s repository:\nlink\nAblation Study Baselines B1: Image Classification:\nA straightforward image classifier based on ResNet-50, fine-tuned to classify group activities using a single frame from a video clip.\nB3: Fine-tuned Person Classification:\nThe ResNet-50 CNN model is deployed on each person. Feature extraction for each crop 2048 features are pooled over all people and then fed to a softmax classifier to recognize group activities in a single frame.\nB4: Temporal Model with Image Features:\nA temporal model that uses image features per clip. Each clip consists of 9 frames, and an LSTM is trained on sequences of 9 steps for each clip.\nB5: Temporal Model with Person Features:\nA temporal extension of the previous baseline (B3) temporal on crops (LSTM on player level), where person-specific features pooled over all individuals to recognize group activities.\nB6: Two-stage Model without LSTM 1:\nIndividual features pooled over all people are fed into an LSTM model to capture group dynamics.\nB7: Two-stage Model without LSTM 2:\nThe full model (V1) trains an LSTM on crop-level data (LSTM on a player level). Clips are extracted: sequences of 9 steps per player for each frame. A max-pooling operation is applied to the players, and LSTM 2 is trained on the frame level.\nB8: Two-stage Hierarchical Model:\nThe full model (V2) trains an LSTM on crop-level data (LSTM on a player level). Clips are extracted as sequences of 9 steps per player for each frame. A max-pooling operation is applied to each player’s team in a dependent way. Features from both teams are concatenated along the feature dimension, and the result is fed to LSTM 2 at the frame level.\nB9: Unified Hierarchical Model:\nIn earlier baselines, person-level and group-level activity losses were addressed independently, leading to a two-stage model. Baseline 9 integrates these processes into a unified, end-to-end training pipeline. This approach enables simultaneous optimization of both person-level and group-level activity classification through a shared gradient flow. Additionally, Baseline 9 employs ResNet34 instead of ResNet50 and GUR instead of LSTM, reducing model complexity and mitigating the risk of overfitting.\nPerformance comparison Original Paper Baselines Score My Scores (Accuracy and F1 Scores) Baseline Accuracy F1 Score Baseline 1 72.66% 72.63% Baseline 3 80.25% 80.24% Baseline 4 76.59% 76.67% Baseline 5 77.04% 77.07% Baseline 6 84.52% 83.99% Baseline 7 89.15% 89.14% Baseline 8 92.30% 92.29% Baseline 9 93.12% 93.11% Interesting Observations Effect of Team Independent Pooling The following confusion matrices from Baseline 5 and Baseline 6 reveal some interesting insights:\nBaseline 5 Confusion Matrix Baseline 6 Confusion Matrix The most frequent confusions occur between: Right winpoint vs. left winpoint Right pass vs. left pass Right set vs. left set Right spike vs. left spike This behavior is likely due to the pooling of the 12 players from both teams when transitioning from the individual/personal level to the frame/group level. By grouping all players into one unit, the model loses valuable geometric information regarding player positions.\nWhen the teams are grouped and processed individually before concatenation, the player position information is retained. This suggests that a more careful handling of player positions could improve model performance, as observed in Baseline 8 and Baseline 9.\nBaseline 8 Confusion Matrix Baseline 9 Confusion Matrix Model Architecture (Baseline 8) The baseline model architecture for temporal group activity classification is designed to integrate individual player features and team-level dynamics over time. This section provides a detailed description of the components and processing stages of the model.\nPlayer-Level Feature Extraction: Individual player features are extracted and processed over time using ResNet-50 and LSTM. Team-Level Feature Integration: Features from both teams are aggregated and processed further using a second LSTM to classify the group activity. 1. Player Activity Temporal Classifier The Person_Activity_Temporal_Classifier is responsible for extracting features for individual players from input sequences of video frames. It consists of the following components:\nResNet-50 Backbone: Pretrained ResNet-50 (excluding the final fully connected layer) is used to extract spatial features for each player from image crops. Layer Normalization: Applied to stabilize and normalize the extracted features. Temporal Modeling with LSTM: An LSTM processes the sequence of features for each player, capturing temporal dependencies. Fully Connected Layers: A series of dense layers map the LSTM outputs to the target activity classes. 2. Group Activity Temporal Classifier The Group_Activity_Temporal_Classifier extends the player-level classifier to incorporate team-level dynamics:\nShared ResNet-50 and LSTM: The ResNet-50 and LSTM from the Person_Activity_Temporal_Classifier are shared, with frozen parameters to leverage pretrained weights. Pooling and Feature Concatenation: ResNet-50 and LSTM outputs for individual players are concatenated along the feature dimension. Features are grouped into two teams (e.g., players 1–6 for Team 1 and players 7–12 for Team 2). An adaptive max-pooling layer aggregates player features within each team. Features from both teams are concatenated. Team-Level LSTM: A second LSTM processes the concatenated team-level features over time, capturing temporal dependencies between team interactions. Classification Layers: Fully connected layers map the LSTM outputs to the final group activity class. Training Configuration Training Platform: The model is trained on Kaggle’s free GPU quota (P100 16 RAM GPU) Notebook. Optimizer: AdamW optimizer with learning rate scheduling. Batch Size: 8 Model Architecture: Hierarchical Group Activity Classifier (Baseline 9) The Hierarchical_Group_Activity_Classifier combines spatial feature extraction, temporal modeling, and hierarchical aggregation to provide predictions at both the individual and group levels.\nFeature Extraction:\nA pretrained ResNet-34 extracts spatial features from individual video frames. Individual-Level Classification:\nExtracted features are normalized (Layer Normalization) and passed through a Gated Recurrent Unit (GRU) to capture temporal dependencies for each bounding box across frames. The temporal output is classified into individual activity classes using a fully connected network comprising multiple layers with normalization, activation, and dropout. Group-Level Classification:\nFeatures from individuals are pooled into team representations using adaptive max pooling, splitting individuals into predefined groups. The pooled features are normalized and passed through another GRU to capture higher-level temporal dynamics at the group level. A similar fully connected network classifies the group activities. Outputs:\nperson_output: Predictions for individual activity classes. group_output: Predictions for group activity classes. Training Configuration Training Platform: The model is trained on Kaggle’s free GPU quota (X2 T4 15 RAM GPU) Notebook. Optimizer: AdamW optimizer with learning rate scheduling. Batch Size: 8 ","wordCount":"1440","inLanguage":"en","image":"http://localhost:1313/projects/Group_Activity_Recognition/cover.png","datePublished":"2025-02-24T00:00:00Z","dateModified":"2025-02-24T00:00:00Z","author":{"@type":"Person","name":"Sherif Ahmed"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/projects/group-activity-recognition/"},"publisher":{"@type":"Organization","name":"Sherif Ahmed","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body class=dark id=top><!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Your Website Title</title><link rel=stylesheet href=path/to/your/styles.css><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML"></script><script type=text/x-mathjax-config>
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
      });
    </script></head><body><header class=header style="position:sticky;top:0;z-index:100;background:var(--theme);box-shadow:0 2px 4px rgba(0,0,0,1%)"><nav class=nav><div class=logo><a href=http://localhost:1313/ class=header-terminal><span class=header-terminal-text></span>
<span class=header-terminal-cursor></span></a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/projects/ title="My Projects"><span>My Projects</span></a></li><li><a href=http://localhost:1313/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header></body></html><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">Group Activity Recognition</h1><div class=post-description>A modern implementation implemented a Hierarchical Deep Temporal Model for Group Activity Recognition, based on the CVPR 2016 paper. Achieved 93% accuracy using a two-stage LSTM architecture to recognize multi-person activities. Conducted ablation studies to evaluate the contributions of various model components and compared performance against 8 baseline models.</div><div class=post-meta><span title='2025-02-24 00:00:00 +0000 UTC'>February 24, 24240</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1440 words&nbsp;·&nbsp;Sherif Ahmed</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/projects/Group_Activity_Recognition/cover.png alt=Group_Activity_Recognition></figure><div class=post-content><p><a href=https://github.com/Sh-31/Group-Activity-Recognition><img alt=Repo loading=lazy src="https://img.shields.io/badge/github-repo-black?logo=github&style=for-the-badge&scale=2"></a></p><p><a href=https://arxiv.org/abs/1607.02643><img alt=arXiv loading=lazy src=https://img.shields.io/badge/arXiv-1607.02643-b31b1b.svg></a></p><h1 id=group-activity-recognition>Group Activity Recognition<a hidden class=anchor aria-hidden=true href=#group-activity-recognition>#</a></h1><h2 id=table-of-contents>Table of Contents<a hidden class=anchor aria-hidden=true href=#table-of-contents>#</a></h2><ol><li><a href=#key-updates>Key Updates</a></li><li><a href=#usage>Usage</a><ul><li><a href=#1-clone-the-repository>Clone the Repository</a></li><li><a href=#2-install-the-required-dependencies>Install Dependencies</a></li><li><a href=#3-download-the-model-checkpoint>Download Model Checkpoint</a></li></ul></li><li><a href=#dataset-overview>Dataset Overview</a><ul><li><a href=#example-annotations>Example Annotations</a></li><li><a href=#train-test-split>Train-Test Split</a></li><li><a href=#dataset-statistics>Dataset Statistics</a></li><li><a href=#dataset-organization>Dataset Organization</a></li><li><a href=#dataset-download-instructions>Dataset Download Instructions</a></li></ul></li><li><a href=#ablation-study>Ablation Study</a><ul><li><a href=#baselines>Baselines</a></li></ul></li><li><a href=#performance-comparison>Performance Comparison</a><ul><li><a href=#original-paper-baselines-score>Original Paper Baselines Score</a></li><li><a href=#my-scores-accuracy-and-f1-scores>My Scores (Accuracy and F1 Scores)</a></li></ul></li><li><a href=#interesting-observations>Interesting Observations</a><ul><li><a href=#effect-of-team-independent-pooling>Effect of Team Independent Pooling</a></li></ul></li><li><a href=#model-architecture-baseline-8>Model Architecture</a></li></ol><h2 id=key-updates>Key Updates<a hidden class=anchor aria-hidden=true href=#key-updates>#</a></h2><ul><li>ResNet50 for feature extraction (replacing AlexNet).</li><li>Ablation studies to analyze model components.</li><li>Implementation of an end-to-end version (Baseline 9).</li><li>Achieve higher performance across every model baseline compared to the original paper.</li><li>Full implementation in Python (original used Caffe).</li></ul><hr><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><hr><h3 id=1-clone-the-repository>1. Clone the Repository<a hidden class=anchor aria-hidden=true href=#1-clone-the-repository>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/Sh-31/Group-Activity-Recognition.git
</span></span></code></pre></td></tr></table></div></div><h3 id=2-install-the-required-dependencies>2. Install the Required Dependencies<a hidden class=anchor aria-hidden=true href=#2-install-the-required-dependencies>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip3 install -r requirements.txt
</span></span></code></pre></td></tr></table></div></div><h3 id=3-download-the-model-checkpoint>3. Download the Model Checkpoint<a hidden class=anchor aria-hidden=true href=#3-download-the-model-checkpoint>#</a></h3><p>This is a manual step that involves downloading the model checkpoint files.</p><h4 id=option-1-use-python-code>Option 1: Use Python Code<a hidden class=anchor aria-hidden=true href=#option-1-use-python-code>#</a></h4><p>Replace the <code>modeling</code> folder with the downloaded folder:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>kagglehub</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Download the latest version</span>
</span></span><span class=line><span class=cl><span class=n>path</span> <span class=o>=</span> <span class=n>kagglehub</span><span class=o>.</span><span class=n>model_download</span><span class=p>(</span><span class=s2>&#34;sherif31/group-activity-recognition/pyTorch/v1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Path to model files:&#34;</span><span class=p>,</span> <span class=n>path</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=option-2-download-directly>Option 2: Download Directly<a hidden class=anchor aria-hidden=true href=#option-2-download-directly>#</a></h4><p>Browse and download the specific checkpoint from Kaggle:<br><a href=https://www.kaggle.com/models/sherif31/group-activity-recognition/pyTorch/v1/1>Group Activity Recognition - PyTorch Checkpoint</a></p><hr><h2 id=dataset-overview>Dataset Overview<a hidden class=anchor aria-hidden=true href=#dataset-overview>#</a></h2><p>The dataset was created using publicly available YouTube volleyball videos. The authors annotated 4,830 frames from 55 videos, categorizing player actions into 9 labels and team activities into 8 labels.</p><h3 id=example-annotations>Example Annotations<a hidden class=anchor aria-hidden=true href=#example-annotations>#</a></h3><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/50f906ad-c68c-4882-b9cf-9200f5a380c7></p><ul><li><strong>Figure</strong>: A frame labeled as &ldquo;Left Spike,&rdquo; with bounding boxes around each player, demonstrating team activity annotations.</li></ul><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/cca9447a-8b40-4330-a11d-dbc0feb230ff></p><h3 id=train-test-split>Train-Test Split<a hidden class=anchor aria-hidden=true href=#train-test-split>#</a></h3><ul><li><strong>Training Set</strong>: 3,493 frames</li><li><strong>Testing Set</strong>: 1,337 frames</li></ul><h3 id=dataset-statistics>Dataset Statistics<a hidden class=anchor aria-hidden=true href=#dataset-statistics>#</a></h3><h4 id=group-activity-labels>Group Activity Labels<a hidden class=anchor aria-hidden=true href=#group-activity-labels>#</a></h4><table><thead><tr><th>Group Activity Class</th><th>Instances</th></tr></thead><tbody><tr><td>Right set</td><td>644</td></tr><tr><td>Right spike</td><td>623</td></tr><tr><td>Right pass</td><td>801</td></tr><tr><td>Right winpoint</td><td>295</td></tr><tr><td>Left winpoint</td><td>367</td></tr><tr><td>Left pass</td><td>826</td></tr><tr><td>Left spike</td><td>642</td></tr><tr><td>Left set</td><td>633</td></tr></tbody></table><h4 id=player-action-labels>Player Action Labels<a hidden class=anchor aria-hidden=true href=#player-action-labels>#</a></h4><table><thead><tr><th>Action Class</th><th>Instances</th></tr></thead><tbody><tr><td>Waiting</td><td>3,601</td></tr><tr><td>Setting</td><td>1,332</td></tr><tr><td>Digging</td><td>2,333</td></tr><tr><td>Falling</td><td>1,241</td></tr><tr><td>Spiking</td><td>1,216</td></tr><tr><td>Blocking</td><td>2,458</td></tr><tr><td>Jumping</td><td>341</td></tr><tr><td>Moving</td><td>5,121</td></tr><tr><td>Standing</td><td>38,696</td></tr></tbody></table><h3 id=dataset-organization>Dataset Organization<a hidden class=anchor aria-hidden=true href=#dataset-organization>#</a></h3><ul><li><strong>Videos</strong>: 55, each assigned a unique ID (0–54).</li><li><strong>Train Videos</strong>: 1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54.</li><li><strong>Validation Videos</strong>: 0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51.</li><li><strong>Test Videos</strong>: 4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47.</li></ul><h3 id=dataset-download-instructions>Dataset Download Instructions<a hidden class=anchor aria-hidden=true href=#dataset-download-instructions>#</a></h3><ol><li>Enable Kaggle&rsquo;s public API. Follow the guide here: <a href=https://www.kaggle.com/docs/api>Kaggle API Documentation</a>.</li><li>Use the provided shell script:</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>  chmod <span class=m>600</span> .kaggle/kaggle.json 
</span></span><span class=line><span class=cl>  chmod +x script/script_download_volleball_dataset.sh
</span></span><span class=line><span class=cl>  .script/script_download_volleball_dataset.sh
</span></span></code></pre></td></tr></table></div></div><p>For further information about dataset, you can check out the paper author&rsquo;s repository:<br><a href=https://github.com/mostafa-saad/deep-activity-rec>link</a></p><hr><h2 id=ablation-study><a href="https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)#:~:text=In%20artificial%20intelligence%20(AI)%2C,resultant%20performance%20of%20the%20system">Ablation Study</a><a hidden class=anchor aria-hidden=true href=#ablation-study>#</a></h2><h3 id=baselines>Baselines<a hidden class=anchor aria-hidden=true href=#baselines>#</a></h3><ul><li><p><strong>B1: Image Classification:</strong><br>A straightforward image classifier based on ResNet-50, fine-tuned to classify group activities using a single frame from a video clip.</p></li><li><p><strong>B3: Fine-tuned Person Classification:</strong><br>The ResNet-50 CNN model is deployed on each person. Feature extraction for each crop 2048 features are pooled over all people and then fed to a softmax classifier to recognize group activities in a single frame.</p></li><li><p><strong>B4: Temporal Model with Image Features:</strong><br>A temporal model that uses image features per clip. Each clip consists of 9 frames, and an LSTM is trained on sequences of 9 steps for each clip.</p></li><li><p><strong>B5: Temporal Model with Person Features:</strong><br>A temporal extension of the previous baseline (B3) temporal on crops (LSTM on player level), where person-specific features pooled over all individuals to recognize group activities.</p></li><li><p><strong>B6: Two-stage Model without LSTM 1:</strong><br>Individual features pooled over all people are fed into an LSTM model to capture group dynamics.</p></li><li><p><strong>B7: Two-stage Model without LSTM 2:</strong><br>The full model (V1) trains an LSTM on crop-level data (LSTM on a player level). Clips are extracted: sequences of 9 steps per player for each frame. A max-pooling operation is applied to the players, and LSTM 2 is trained on the frame level.</p></li><li><p><strong>B8: Two-stage Hierarchical Model:</strong><br>The full model (V2) trains an LSTM on crop-level data (LSTM on a player level). Clips are extracted as sequences of 9 steps per player for each frame. A max-pooling operation is applied to each player&rsquo;s team in a dependent way. Features from both teams are concatenated along the feature dimension, and the result is fed to LSTM 2 at the frame level.</p></li><li><p><strong>B9: Unified Hierarchical Model:</strong><br>In earlier baselines, person-level and group-level activity losses were addressed independently, leading to a two-stage model. Baseline 9 integrates these processes into a unified, end-to-end training pipeline. This approach enables simultaneous optimization of both person-level and group-level activity classification through a shared gradient flow. Additionally, Baseline 9 employs <code>ResNet34</code> instead of <code>ResNet50</code> and <code>GUR</code> instead of <code>LSTM</code>, <strong>reducing model complexity and mitigating the risk of overfitting</strong>.</p></li></ul><hr><h2 id=performance-comparison>Performance comparison<a hidden class=anchor aria-hidden=true href=#performance-comparison>#</a></h2><h3 id=original-paper-baselines-score>Original Paper Baselines Score<a hidden class=anchor aria-hidden=true href=#original-paper-baselines-score>#</a></h3><p><img alt={83C0D210-27DA-4A7F-8126-D9407823B766} loading=lazy src=https://github.com/user-attachments/assets/c62ee368-8027-4e83-a5a4-687b7adebe5a></p><h3 id=my-scores-accuracy-and-f1-scores>My Scores (Accuracy and F1 Scores)<a hidden class=anchor aria-hidden=true href=#my-scores-accuracy-and-f1-scores>#</a></h3><table><thead><tr><th><strong>Baseline</strong></th><th><strong>Accuracy</strong></th><th><strong>F1 Score</strong></th></tr></thead><tbody><tr><td>Baseline 1</td><td>72.66%</td><td>72.63%</td></tr><tr><td>Baseline 3</td><td>80.25%</td><td>80.24%</td></tr><tr><td>Baseline 4</td><td>76.59%</td><td>76.67%</td></tr><tr><td>Baseline 5</td><td>77.04%</td><td>77.07%</td></tr><tr><td>Baseline 6</td><td>84.52%</td><td>83.99%</td></tr><tr><td>Baseline 7</td><td>89.15%</td><td>89.14%</td></tr><tr><td>Baseline 8</td><td>92.30%</td><td>92.29%</td></tr><tr><td>Baseline 9</td><td>93.12%</td><td>93.11%</td></tr></tbody></table><hr><h2 id=interesting-observations>Interesting Observations<a hidden class=anchor aria-hidden=true href=#interesting-observations>#</a></h2><h3 id=effect-of-team-independent-pooling>Effect of Team Independent Pooling<a hidden class=anchor aria-hidden=true href=#effect-of-team-independent-pooling>#</a></h3><p>The following confusion matrices from Baseline 5 and Baseline 6 reveal some interesting insights:</p><h4 id=baseline-5-confusion-matrix>Baseline 5 Confusion Matrix<a hidden class=anchor aria-hidden=true href=#baseline-5-confusion-matrix>#</a></h4><img src=https://raw.githubusercontent.com/Sh-31/Group-Activity-Recognition/refs/heads/main/modeling/baseline%205/outputs/Group_Activity_Baseline_5_eval_on_testset_confusion_matrix.png alt="Baseline 5 confusion matrix" width=100%><h4 id=baseline-6-confusion-matrix>Baseline 6 Confusion Matrix<a hidden class=anchor aria-hidden=true href=#baseline-6-confusion-matrix>#</a></h4><img src=https://raw.githubusercontent.com/Sh-31/Group-Activity-Recognition/refs/heads/main/modeling/baseline%206/outputs/Group_Activity_Baseline_6_eval_on_testset_confusion_matrix.png alt="Baseline 6 confusion matrix" width=100%><ul><li>The most frequent confusions occur between:<ul><li>Right winpoint vs. left winpoint</li><li>Right pass vs. left pass</li><li>Right set vs. left set</li><li>Right spike vs. left spike</li></ul></li></ul><p>This behavior is likely due to the pooling of the 12 players from both teams when transitioning from the individual/personal level to the frame/group level. By grouping all players into one unit, the model loses valuable geometric information regarding player positions.</p><p>When the teams are grouped and processed individually before concatenation, the player position information is retained. This suggests that a more careful handling of player positions could improve model performance, as observed in Baseline 8 and Baseline 9.</p><h4 id=baseline-8-confusion-matrix>Baseline 8 Confusion Matrix<a hidden class=anchor aria-hidden=true href=#baseline-8-confusion-matrix>#</a></h4><img src=https://raw.githubusercontent.com/Sh-31/Group-Activity-Recognition/refs/heads/main/modeling/baseline%208/outputs/Group_Activity_Baseline_8_eval_on_testset_confusion_matrix.png alt="Baseline 8 confusion matrix" width=100%><h4 id=baseline-9-confusion-matrix>Baseline 9 Confusion Matrix<a hidden class=anchor aria-hidden=true href=#baseline-9-confusion-matrix>#</a></h4><img src=https://raw.githubusercontent.com/Sh-31/Group-Activity-Recognition/refs/heads/main/modeling/baseline%209%20(end%20to%20end)/outputs/Group_Activity_Baseline_9_eval_on_testset_confusion_matrix.png alt="Baseline 9 confusion matrix" width=100%><hr><h3 id=model-architecture-baseline-8>Model Architecture (Baseline 8)<a hidden class=anchor aria-hidden=true href=#model-architecture-baseline-8>#</a></h3><p>The baseline model architecture for temporal group activity classification is designed to integrate individual player features and team-level dynamics over time. This section provides a detailed description of the components and processing stages of the model.</p><ol><li><strong>Player-Level Feature Extraction</strong>: Individual player features are extracted and processed over time using ResNet-50 and LSTM.</li><li><strong>Team-Level Feature Integration</strong>: Features from both teams are aggregated and processed further using a second LSTM to classify the group activity.</li></ol><h4 id=1-player-activity-temporal-classifier><strong>1. Player Activity Temporal Classifier</strong><a hidden class=anchor aria-hidden=true href=#1-player-activity-temporal-classifier>#</a></h4><p>The <code>Person_Activity_Temporal_Classifier</code> is responsible for extracting features for individual players from input sequences of video frames. It consists of the following components:</p><ul><li><strong>ResNet-50 Backbone</strong>: Pretrained ResNet-50 (excluding the final fully connected layer) is used to extract spatial features for each player from image crops.</li><li><strong>Layer Normalization</strong>: Applied to stabilize and normalize the extracted features.</li><li><strong>Temporal Modeling with LSTM</strong>: An LSTM processes the sequence of features for each player, capturing temporal dependencies.</li><li><strong>Fully Connected Layers</strong>: A series of dense layers map the LSTM outputs to the target activity classes.</li></ul><h4 id=2-group-activity-temporal-classifier><strong>2. Group Activity Temporal Classifier</strong><a hidden class=anchor aria-hidden=true href=#2-group-activity-temporal-classifier>#</a></h4><p>The <code>Group_Activity_Temporal_Classifier</code> extends the player-level classifier to incorporate team-level dynamics:</p><ul><li><strong>Shared ResNet-50 and LSTM</strong>: The ResNet-50 and LSTM from the <code>Person_Activity_Temporal_Classifier</code> are shared, with frozen parameters to leverage pretrained weights.</li><li><strong>Pooling and Feature Concatenation</strong>:<ul><li>ResNet-50 and LSTM outputs for individual players are concatenated along the feature dimension.</li><li>Features are grouped into two teams (e.g., players 1–6 for Team 1 and players 7–12 for Team 2).</li><li>An adaptive max-pooling layer aggregates player features within each team.</li><li>Features from both teams are concatenated.</li></ul></li><li><strong>Team-Level LSTM</strong>: A second LSTM processes the concatenated team-level features over time, capturing temporal dependencies between team interactions.</li><li><strong>Classification Layers</strong>: Fully connected layers map the LSTM outputs to the final group activity class.</li></ul><h4 id=training-configuration>Training Configuration<a hidden class=anchor aria-hidden=true href=#training-configuration>#</a></h4><ul><li><strong>Training Platform</strong>: The model is trained on Kaggle&rsquo;s free GPU quota (P100 16 RAM GPU) <a href=https://www.kaggle.com/code/sherif31/gar-baseline-8>Notebook</a>.</li><li><strong>Optimizer</strong>: AdamW optimizer with learning rate scheduling.</li><li><strong>Batch Size:</strong> 8</li></ul><hr><h3 id=model-architecture-hierarchical-group-activity-classifier-baseline-9>Model Architecture: Hierarchical Group Activity Classifier (Baseline 9)<a hidden class=anchor aria-hidden=true href=#model-architecture-hierarchical-group-activity-classifier-baseline-9>#</a></h3><p>The <code>Hierarchical_Group_Activity_Classifier</code> combines spatial feature extraction, temporal modeling, and hierarchical aggregation to provide predictions at both the individual and group levels.</p><ol><li><p><strong>Feature Extraction</strong>:</p><ul><li>A pretrained ResNet-34 extracts spatial features from individual video frames.</li></ul></li><li><p><strong>Individual-Level Classification</strong>:</p><ul><li>Extracted features are normalized (<code>Layer Normalization</code>) and passed through a Gated Recurrent Unit (GRU) to capture temporal dependencies for each bounding box across frames.</li><li>The temporal output is classified into individual activity classes using a fully connected network comprising multiple layers with normalization, activation, and dropout.</li></ul></li><li><p><strong>Group-Level Classification</strong>:</p><ul><li>Features from individuals are pooled into team representations using adaptive max pooling, splitting individuals into predefined groups.</li><li>The pooled features are normalized and passed through another GRU to capture higher-level temporal dynamics at the group level.</li><li>A similar fully connected network classifies the group activities.</li></ul></li><li><p><strong>Outputs</strong>:</p><ul><li><code>person_output</code>: Predictions for individual activity classes.</li><li><code>group_output</code>: Predictions for group activity classes.</li></ul></li></ol><h4 id=training-configuration-1>Training Configuration<a hidden class=anchor aria-hidden=true href=#training-configuration-1>#</a></h4><ul><li><strong>Training Platform</strong>: The model is trained on Kaggle&rsquo;s free GPU quota (X2 T4 15 RAM GPU) <a href=https://www.kaggle.com/code/sherif31/gar-baseline-9-22af56>Notebook</a>.</li><li><strong>Optimizer</strong>: AdamW optimizer with learning rate scheduling.</li><li><strong>Batch Size:</strong> 8</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/paper-implementation/>Paper-Implementation</a></li><li><a href=http://localhost:1313/tags/cvpr-2016/>CVPR-2016</a></li><li><a href=http://localhost:1313/tags/distributed-training/>Distributed-Training</a></li><li><a href=http://localhost:1313/tags/torch/>Torch</a></li><li><a href=http://localhost:1313/tags/two-stage-arch/>Two-Stage Arch</a></li><li><a href=http://localhost:1313/tags/spatial-temporal/>Spatial-Temporal</a></li><li><a href=http://localhost:1313/tags/multi-person-recognition/>Multi-Person-Recognition</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/projects/courseta/><span class=title>« Prev</span><br><span>CourseTA</span>
</a><a class=next href=http://localhost:1313/projects/imacap/><span class=title>Next »</span><br><span>ImgCap</span></a></nav></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><style>.terminal-line-container{display:flex;justify-content:center;margin-top:5rem;margin-bottom:2rem}.terminal-line{font-family:monospace;background-color:#212121;padding:15px;border-radius:5px;color:#fafafa;display:flex;align-items:center;min-width:300px}.terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:#fafafa;animation:blink 1s step-end infinite;margin-left:4px}@keyframes blink{from,to{background-color:transparent}50%{background-color:#fafafa}}.header-terminal{font-family:courier new,Courier,monospace;padding:6px 12px;color:rgba(255,255,255,.9);display:flex;align-items:center;font-size:.95rem;margin-right:16px;text-decoration:none;transition:all .2s ease;letter-spacing:-.5px}.header-terminal:hover{transform:translateY(-1px);opacity:.8}.header-terminal-text{white-space:nowrap;font-weight:600;color:rgba(255,255,255,.9)}.header-terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:rgba(255,255,255,.9);animation:blink .8s step-end infinite;margin-left:4px;opacity:.9}@media(max-width:600px){.header-terminal{font-size:.85rem;padding:4px 8px}}.footer-content{display:flex;flex-direction:row;align-items:center;justify-content:center;padding:0rem 1;gap:.5rem;text-align:center}.copyright{color:var(--secondary);font-size:.9rem}.author{color:var(--primary);font-size:1rem;font-weight:500}.entry-content .profile-mode-image img{border-radius:50%;object-fit:cover;aspect-ratio:1;border:3px solid var(--primary);transition:transform .3s ease,border-color .3s ease}.entry-content .profile-mode-image img:hover{transform:scale(1.05);border-color:var(--secondary)}</style><script>(function(){const t="cd /home/about",n=document.getElementById("terminal-text");if(!n)return;let e=0;function s(){if(e<t.length)n.textContent+=t.charAt(e),e++,setTimeout(s,120);else{const e=document.querySelector(".terminal-cursor");e&&(e.style.animation="none",e.style.backgroundColor="#fafafa")}}const o=new IntersectionObserver(e=>{e.forEach(e=>{e.isIntersecting&&(setTimeout(s,500),o.unobserve(e.target))})}),i=document.querySelector(".terminal-line");i&&o.observe(i)})();function updateTerminalPath(){const t=window.location.pathname;let e="> cd";t==="/"?e+=" /home":t.endsWith("/")?e+=" /home"+t.slice(0,-1):e+=" /home"+t;const n=document.querySelector(".header-terminal-text");if(n){let t=0;n.textContent="> ";function o(){t<e.length&&(n.textContent=e.substring(0,t+1),t++,setTimeout(o,35))}o()}const s=document.getElementById("terminal-text");if(s){s.textContent="";let t=0;function i(){t<e.length&&(s.textContent=e.substring(0,t+1),t++,setTimeout(i,120))}setTimeout(i,500)}}document.addEventListener("DOMContentLoaded",updateTerminalPath),window.addEventListener("popstate",updateTerminalPath)</script><footer class=footer><div class=footer-content><div class=copyright>© 2025</div><div class=author>Sherif Ahmed</div></div></footer></body></html>