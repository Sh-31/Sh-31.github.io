<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ImgCap | Sherif Ahmed</title><meta name=keywords content="CNN,RNN,Beam-Search,Attention mechanism,NLP,Torch,Teacher Forcing"><meta name=description content="ImgCap is an image captioning model designed to automatically generate descriptive captions for images. It has two versions one based on a CNN-LSTM architecture, and the other an enhanced version that integrates a CNN-LSTM with an Attention mechanism. To further enhance the quality and coherence of generated captions, beam search is implemented during the inference stage."><meta name=author content="Sherif Ahmed"><link rel=canonical href=https://sh-31.github.io/projects/imacap/><link crossorigin=anonymous href=/assets/css/stylesheet.023a79c09a6253ba6d2a259b4c3db25df8cce828b8b0e2cc06c6675372eb7b43.css integrity="sha256-Ajp5wJpiU7ptKiWbTD2yXfjM6Ci4sOLMBsZnU3Lre0M=" rel="preload stylesheet" as=style><link rel=icon href=https://sh-31.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://sh-31.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://sh-31.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://sh-31.github.io/apple-touch-icon.png><link rel=mask-icon href=https://sh-31.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://sh-31.github.io/projects/imacap/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://sh-31.github.io/projects/imacap/"><meta property="og:site_name" content="Sherif Ahmed"><meta property="og:title" content="ImgCap"><meta property="og:description" content="ImgCap is an image captioning model designed to automatically generate descriptive captions for images. It has two versions one based on a CNN-LSTM architecture, and the other an enhanced version that integrates a CNN-LSTM with an Attention mechanism. To further enhance the quality and coherence of generated captions, beam search is implemented during the inference stage."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2024-09-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-10T00:00:00+00:00"><meta property="article:tag" content="CNN"><meta property="article:tag" content="RNN"><meta property="article:tag" content="Beam-Search"><meta property="article:tag" content="Attention Mechanism"><meta property="article:tag" content="NLP"><meta property="article:tag" content="Torch"><meta property="og:image" content="https://sh-31.github.io/projects/ImgCap/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sh-31.github.io/projects/ImgCap/cover.png"><meta name=twitter:title content="ImgCap"><meta name=twitter:description content="ImgCap is an image captioning model designed to automatically generate descriptive captions for images. It has two versions one based on a CNN-LSTM architecture, and the other an enhanced version that integrates a CNN-LSTM with an Attention mechanism. To further enhance the quality and coherence of generated captions, beam search is implemented during the inference stage."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://sh-31.github.io/projects/"},{"@type":"ListItem","position":2,"name":"ImgCap","item":"https://sh-31.github.io/projects/imacap/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ImgCap","name":"ImgCap","description":"ImgCap is an image captioning model designed to automatically generate descriptive captions for images. It has two versions one based on a CNN-LSTM architecture, and the other an enhanced version that integrates a CNN-LSTM with an Attention mechanism. To further enhance the quality and coherence of generated captions, beam search is implemented during the inference stage.","keywords":["CNN","RNN","Beam-Search","Attention mechanism","NLP","Torch","Teacher Forcing"],"articleBody":" Usage Clone the repository:\n1 git clone https://github.com/Sh-31/ImgCap.git Install the required dependencies:\n1 2 pip3 install -r requirements.txt python3 -q -m spacy download en_core_web_sm Download the model checkpoint (manual step):\nImgCap (CNN + LSTM): Download checkpoint ImgCap (CNN + LSTM + Attention): Download checkpoint Place the model checkpoint in the appropriate directory: For CNN + LSTM + Attention: ImgCap/trainning/checkpoints/attention For CNN + LSTM: ImgCap/trainning/checkpoints Run the main script (Gradio GUI for inference):\n1 python3 main.py Alternatively, you can use the model directly on Hugging Face Spaces: ImgCap on Hugging Face\nSample Output Dataset Flickr30k: The Flickr30k dataset consists of 30,000 images, each accompanied by five captions. It provides a wide variety of scenes and objects, making it ideal for diverse image captioning tasks.\nTo download the dataset, follow these steps:\nEnable Kaggle’s public API by following the instructions here: Kaggle API. Run the following command to download the dataset: 1 kaggle datasets download -d hsankesara/flickr-image-dataset -p /teamspace/studios/this_studio/data/Flickr30 Additionally, I’ve documented a similar image captioning dataset, which you can review here: Image Caption Documentation.\nModel Architecture Comparison and Details The model architectures compared in this report consist of two versions of the ImgCap model, each with different configurations. The models were trained using Float16 precision and optimized with torch.compile for improved training efficiency on an L4 24GB RAM GPU.\nKey Differences Number of Parameters:\nImgCap with Attention: This model incorporates an additional attention mechanism that increases the parameter count. Specifically, the attention layer adds about 3.15M parameters, bringing the total to 85.79M. Out of these, 36.72M are trainable, with the rest being frozen in the ResNet50 encoder. ImgCap without Attention: The model without the attention mechanism has 52.89M total parameters, with 29.38M being trainable, as it simplifies the decoder by removing the attention layers. CNN Encoder (ResNet50) Freezing Strategy:\nBoth models use ResNet50 as the CNN encoder. The convolutional layers in ResNet50 are frozen to reduce computational overhead and focus training on the LSTM-based decoder. Only the fully connected layers at the end of ResNet50 are trainable in both models. LSTM Decoder and Embedding:\nBoth models use an LSTM-based decoder with trainable embedding layers. The LSTM decoder with attention concatenates the context vectors obtained from the attention mechanism, while the non-attention model directly processes image features via projection layers. The embedding dimension, hidden size, and number of layers in the LSTM remain consistent across both models. Vocabulary Construction:\nCaptions Tokenization: Captions are tokenized using spaCy, which splits captions into tokens. These tokens are then used to build the vocabulary (vocab size 4096). Vocabulary Content: The vocabulary includes special tokens (, , , ) and tokens derived from the captions. Individual English alphabet characters and spaces are also added to the vocabulary to handle out-of-vocabulary words or character-level tokenization. Tokenization: Each caption is tokenized into tokens that are then mapped to indices in the vocabulary. Encoding: Tokens are converted to indices, starting with , followed by the token indices, and ending with . This encoding helps the LSTM decoder understand the sequence of words. Teacher Forcing:\nTeacher Forcing Ratio: During training, both models used a teacher forcing ratio of 0.90, meaning that 90% of the time, the ground truth caption tokens were fed into the decoder during sequence generation, while the remaining 10% relied on the model’s predictions. Training Configuration:\nBoth models were trained using mixed precision (Float16) to improve memory efficiency and training speed. The training was executed on an L4 24GB RAM GPU using torch.compile for improved runtime optimizations, enabling faster convergence and better GPU utilization. Parameter Comparison Component ImgCap with Attention ImgCap without Attention Total Parameters 85.79M 52.89M Trainable Parameters 36.72M 29.38M Non-trainable Parameters 49.07M 23.51M ImgCap with Attention ImgCap without Attention The model with attention has more trainable parameters and introduces a more complex mechanism for context generation, leading to improved performance in captioning tasks, as seen in the evaluation metrics. However, due to the larger number of parameters, the model may require longer training time to fully converge. Model Evaluation Model Epoch Beam Width BLEU-1 BLEU-2 BLEU-3 BLEU-4 CIDEr ImgCap without Attention 40 5 0.37 0.22 0.14 0.09 0.41 ImgCap with Attention 30 5 0.3959 0.2464 0.1619 0.1077 0.6213 Note: The models are still undertrained, and in theory, their accuracy is expected to improve with further training. Extending the number of epochs could lead to higher BLEU and CIDEr scores, particularly for the attention-based model, which already shows a performance boost.\nFuture Work In the next phase, I plan to explore the Vision Transformer (ViT) architecture to develop a new variant of the ImgCap model. This variant will scale more effectively for complex visual understanding tasks. Additionally, I aim to expand the model’s capabilities by training it for multilingual captioning in both English and Arabic.\n","wordCount":"788","inLanguage":"en","image":"https://sh-31.github.io/projects/ImgCap/cover.png","datePublished":"2024-09-10T00:00:00Z","dateModified":"2024-09-10T00:00:00Z","author":{"@type":"Person","name":"Sherif Ahmed"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://sh-31.github.io/projects/imacap/"},"publisher":{"@type":"Organization","name":"Sherif Ahmed","logo":{"@type":"ImageObject","url":"https://sh-31.github.io/favicon.ico"}}}</script></head><body class=dark id=top><header class=header style="position:sticky;top:0;z-index:100;background:var(--theme);box-shadow:0 2px 4px rgba(0,0,0,1%)"><nav class=nav><div class=logo><a href=https://sh-31.github.io/ class=header-terminal><span class=header-terminal-text></span>
<span class=header-terminal-cursor></span></a><div class=logo-switches></div></div><ul id=menu><li><a href=https://sh-31.github.io/ title=Home><span>Home</span></a></li><li><a href=https://sh-31.github.io/projects/ title="My Projects"><span>My Projects</span></a></li><li><a href=https://sh-31.github.io/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=https://sh-31.github.io/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://sh-31.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://sh-31.github.io/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">ImgCap</h1><div class=post-description>ImgCap is an image captioning model designed to automatically generate descriptive captions for images. It has two versions one based on a CNN-LSTM architecture, and the other an enhanced version that integrates a CNN-LSTM with an Attention mechanism. To further enhance the quality and coherence of generated captions, beam search is implemented during the inference stage.</div><div class=post-meta><span title='2024-09-10 00:00:00 +0000 UTC'>September 10, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;788 words&nbsp;·&nbsp;Sherif Ahmed</div></header><figure class=entry-cover><img loading=eager src=https://sh-31.github.io/projects/ImgCap/cover.png alt=ImgCap></figure><div class=post-content><p><a href=https://github.com/Sh-31/ImgCap><img alt=Repo loading=lazy src="https://img.shields.io/badge/github-repo-black?logo=github&style=for-the-badge&scale=2"></a> <a href=https://huggingface.co/spaces/shredder-31/ImgCap><img alt=Space loading=lazy src="https://img.shields.io/badge/%F0%9F%A4%97-Space-yellow?style=for-the-badge&scale=2"></a></p><hr><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><hr><ol><li><p><strong>Clone the repository:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone https://github.com/Sh-31/ImgCap.git
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>Install the required dependencies:</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip3 install -r requirements.txt
</span></span><span class=line><span class=cl>python3 -q -m spacy download en_core_web_sm
</span></span></code></pre></td></tr></table></div></div></li><li><p><strong>Download the model checkpoint (manual step):</strong></p><ul><li><strong>ImgCap (CNN + LSTM)</strong>: <a href=https://huggingface.co/spaces/shredder-31/ImgCap/resolve/main/checkpoint_epoch_40.pth>Download checkpoint</a></li><li><strong>ImgCap (CNN + LSTM + Attention)</strong>: <a href=https://huggingface.co/spaces/shredder-31/ImgCap/resolve/main/checkpoint_epoch_30.pth>Download checkpoint</a></li><li>Place the model checkpoint in the appropriate directory:<ul><li>For CNN + LSTM + Attention: <code>ImgCap/trainning/checkpoints/attention</code></li><li>For CNN + LSTM: <code>ImgCap/trainning/checkpoints</code></li></ul></li></ul></li><li><p><strong>Run the main script (Gradio GUI for inference):</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python3 main.py
</span></span></code></pre></td></tr></table></div></div></li></ol><p>Alternatively, you can use the model directly on Hugging Face Spaces: <a href=https://huggingface.co/spaces/shredder-31/ImgCap>ImgCap on Hugging Face</a></p><hr><h2 id=sample-output>Sample Output<a hidden class=anchor aria-hidden=true href=#sample-output>#</a></h2><hr><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/95d9ad9a-8050-48ce-81f1-4da498fa7a65></p><hr><h2 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h2><hr><h4 id=flickr30k>Flickr30k:<a hidden class=anchor aria-hidden=true href=#flickr30k>#</a></h4><p>The Flickr30k dataset consists of 30,000 images, each accompanied by five captions. It provides a wide variety of scenes and objects, making it ideal for diverse image captioning tasks.</p><p>To download the dataset, follow these steps:</p><ol><li>Enable Kaggle’s public API by following the instructions here: <a href=https://www.kaggle.com/docs/api>Kaggle API</a>.</li><li>Run the following command to download the dataset:<div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>kaggle datasets download -d hsankesara/flickr-image-dataset -p /teamspace/studios/this_studio/data/Flickr30
</span></span></code></pre></td></tr></table></div></div></li></ol><p>Additionally, I’ve documented a similar image captioning dataset, which you can review here: <a href="https://docs.google.com/document/d/1eYDuhT2VLufy9Uk5FZBUsgkt6_HCcqjquekvh2XyJzk/edit?usp=sharing">Image Caption Documentation</a>.</p><hr><h2 id=model-architecture-comparison-and-details>Model Architecture Comparison and Details<a hidden class=anchor aria-hidden=true href=#model-architecture-comparison-and-details>#</a></h2><hr><p>The model architectures compared in this report consist of two versions of the <strong>ImgCap</strong> model, each with different configurations. The models were trained using <strong>Float16</strong> precision and optimized with <strong>torch.compile</strong> for improved training efficiency on an <strong>L4 24GB RAM GPU</strong>.</p><h3 id=key-differences>Key Differences<a hidden class=anchor aria-hidden=true href=#key-differences>#</a></h3><ol><li><p><strong>Number of Parameters:</strong></p><ul><li><strong>ImgCap with Attention</strong>: This model incorporates an additional attention mechanism that increases the parameter count. Specifically, the attention layer adds about <strong>3.15M</strong> parameters, bringing the total to <strong>85.79M</strong>. Out of these, <strong>36.72M</strong> are trainable, with the rest being frozen in the ResNet50 encoder.</li><li><strong>ImgCap without Attention</strong>: The model without the attention mechanism has <strong>52.89M</strong> total parameters, with <strong>29.38M</strong> being trainable, as it simplifies the decoder by removing the attention layers.</li></ul></li><li><p><strong>CNN Encoder (ResNet50) Freezing Strategy:</strong></p><ul><li>Both models use <strong>ResNet50</strong> as the CNN encoder. The convolutional layers in ResNet50 are <strong>frozen</strong> to reduce computational overhead and focus training on the LSTM-based decoder. Only the fully connected layers at the end of ResNet50 are trainable in both models.</li></ul></li><li><p><strong>LSTM Decoder and Embedding:</strong></p><ul><li>Both models use an LSTM-based decoder with trainable embedding layers. The LSTM decoder with attention concatenates the context vectors obtained from the attention mechanism, while the non-attention model directly processes image features via projection layers.</li><li>The embedding dimension, hidden size, and number of layers in the LSTM remain consistent across both models.</li></ul></li><li><p><strong>Vocabulary Construction</strong>:</p><ul><li><strong>Captions Tokenization</strong>: Captions are tokenized using spaCy, which splits captions into tokens. These tokens are then used to build the vocabulary (vocab size 4096).</li><li><strong>Vocabulary Content</strong>: The vocabulary includes special tokens (<code>&lt;unk></code>, <code>&lt;pad></code>, <code>&lt;sos></code>, <code>&lt;eos></code>) and tokens derived from the captions. Individual English alphabet characters and spaces are also added to the vocabulary to handle out-of-vocabulary words or character-level tokenization.</li><li><strong>Tokenization</strong>: Each caption is tokenized into tokens that are then mapped to indices in the vocabulary.</li><li><strong>Encoding</strong>: Tokens are converted to indices, starting with <code>&lt;sos></code>, followed by the token indices, and ending with <code>&lt;eos></code>. This encoding helps the LSTM decoder understand the sequence of words.</li></ul></li><li><p><strong><a href=https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/>Teacher Forcing</a>:</strong></p><ul><li><strong>Teacher Forcing Ratio</strong>: During training, both models used a <strong>teacher forcing ratio of 0.90</strong>, meaning that 90% of the time, the ground truth caption tokens were fed into the decoder during sequence generation, while the remaining 10% relied on the model&rsquo;s predictions.</li></ul></li><li><p><strong>Training Configuration:</strong></p><ul><li>Both models were trained using <strong>mixed precision (Float16)</strong> to improve memory efficiency and training speed.</li><li>The training was executed on an <strong>L4 24GB RAM GPU</strong> using <strong>torch.compile</strong> for improved runtime optimizations, enabling faster convergence and better GPU utilization.</li></ul></li></ol><h3 id=parameter-comparison>Parameter Comparison<a hidden class=anchor aria-hidden=true href=#parameter-comparison>#</a></h3><table><thead><tr><th>Component</th><th>ImgCap with Attention</th><th>ImgCap without Attention</th></tr></thead><tbody><tr><td><strong>Total Parameters</strong></td><td>85.79M</td><td>52.89M</td></tr><tr><td><strong>Trainable Parameters</strong></td><td>36.72M</td><td>29.38M</td></tr><tr><td><strong>Non-trainable Parameters</strong></td><td>49.07M</td><td>23.51M</td></tr></tbody></table><h3 id=imgcap-with-attention>ImgCap with Attention<a hidden class=anchor aria-hidden=true href=#imgcap-with-attention>#</a></h3><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/75f74bfd-819d-40ff-afa5-44756d3b340d></p><h3 id=imgcap-without-attention>ImgCap without Attention<a hidden class=anchor aria-hidden=true href=#imgcap-without-attention>#</a></h3><p><img alt=image loading=lazy src=https://github.com/user-attachments/assets/c8adcaa6-7dba-4805-bf0d-142d210a8389></p><ul><li>The model with attention has more trainable parameters and introduces a more complex mechanism for context generation, leading to improved performance in captioning tasks, as seen in the evaluation metrics. However, due to the larger number of parameters, the model may require longer training time to fully converge.</li></ul><hr><h2 id=model-evaluation>Model Evaluation<a hidden class=anchor aria-hidden=true href=#model-evaluation>#</a></h2><hr><table><thead><tr><th>Model</th><th>Epoch</th><th>Beam Width</th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>CIDEr</th></tr></thead><tbody><tr><td><strong>ImgCap without Attention</strong></td><td>40</td><td>5</td><td>0.37</td><td>0.22</td><td>0.14</td><td>0.09</td><td>0.41</td></tr><tr><td><strong>ImgCap with Attention</strong></td><td>30</td><td>5</td><td>0.3959</td><td>0.2464</td><td>0.1619</td><td>0.1077</td><td>0.6213</td></tr></tbody></table><p>Note: The models are still undertrained, and in theory, their accuracy is expected to improve with further training. Extending the number of epochs could lead to higher BLEU and CIDEr scores, particularly for the attention-based model, which already shows a performance boost.</p><hr><h2 id=future-work>Future Work<a hidden class=anchor aria-hidden=true href=#future-work>#</a></h2><hr><p>In the next phase, I plan to explore the <strong>Vision Transformer (ViT)</strong> architecture to develop a new variant of the <strong>ImgCap</strong> model. This variant will scale more effectively for complex visual understanding tasks. Additionally, I aim to expand the model&rsquo;s capabilities by training it for <strong>multilingual captioning</strong> in both <strong>English</strong> and <strong>Arabic</strong>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://sh-31.github.io/tags/cnn/>CNN</a></li><li><a href=https://sh-31.github.io/tags/rnn/>RNN</a></li><li><a href=https://sh-31.github.io/tags/beam-search/>Beam-Search</a></li><li><a href=https://sh-31.github.io/tags/attention-mechanism/>Attention Mechanism</a></li><li><a href=https://sh-31.github.io/tags/nlp/>NLP</a></li><li><a href=https://sh-31.github.io/tags/torch/>Torch</a></li><li><a href=https://sh-31.github.io/tags/teacher-forcing/>Teacher Forcing</a></li></ul><nav class=paginav><a class=prev href=https://sh-31.github.io/projects/group-activity-recognition/><span class=title>« Prev</span><br><span>Group Activity Recognition</span>
</a><a class=next href=https://sh-31.github.io/projects/credit-card-fraud-detection/><span class=title>Next »</span><br><span>Credit Card Fraud Detection</span></a></nav></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><style>.terminal-line-container{display:flex;justify-content:center;margin-top:5rem;margin-bottom:2rem}.terminal-line{font-family:monospace;background-color:#212121;padding:15px;border-radius:5px;color:#fafafa;display:flex;align-items:center;min-width:300px}.terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:#fafafa;animation:blink 1s step-end infinite;margin-left:4px}@keyframes blink{from,to{background-color:transparent}50%{background-color:#fafafa}}.header-terminal{font-family:courier new,Courier,monospace;padding:6px 12px;color:rgba(255,255,255,.9);display:flex;align-items:center;font-size:.95rem;margin-right:16px;text-decoration:none;transition:all .2s ease;letter-spacing:-.5px}.header-terminal:hover{transform:translateY(-1px);opacity:.8}.header-terminal-text{white-space:nowrap;font-weight:600;color:rgba(255,255,255,.9)}.header-terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:rgba(255,255,255,.9);animation:blink .8s step-end infinite;margin-left:4px;opacity:.9}@media(max-width:600px){.header-terminal{font-size:.85rem;padding:4px 8px}}.footer-content{display:flex;flex-direction:row;align-items:center;justify-content:center;padding:0rem 1;gap:.5rem;text-align:center}.copyright{color:var(--secondary);font-size:.9rem}.author{color:var(--primary);font-size:1rem;font-weight:500}.entry-content .profile-mode-image img{border-radius:50%;object-fit:cover;aspect-ratio:1;border:3px solid var(--primary);transition:transform .3s ease,border-color .3s ease}.entry-content .profile-mode-image img:hover{transform:scale(1.05);border-color:var(--secondary)}</style><script>(function(){const t="cd /home/about",n=document.getElementById("terminal-text");if(!n)return;let e=0;function s(){if(e<t.length)n.textContent+=t.charAt(e),e++,setTimeout(s,120);else{const e=document.querySelector(".terminal-cursor");e&&(e.style.animation="none",e.style.backgroundColor="#fafafa")}}const o=new IntersectionObserver(e=>{e.forEach(e=>{e.isIntersecting&&(setTimeout(s,500),o.unobserve(e.target))})}),i=document.querySelector(".terminal-line");i&&o.observe(i)})();function updateTerminalPath(){const t=window.location.pathname;let e="> cd";t==="/"?e+=" /home":t.endsWith("/")?e+=" /home"+t.slice(0,-1):e+=" /home"+t;const n=document.querySelector(".header-terminal-text");if(n){let t=0;n.textContent="> ";function o(){t<e.length&&(n.textContent=e.substring(0,t+1),t++,setTimeout(o,35))}o()}const s=document.getElementById("terminal-text");if(s){s.textContent="";let t=0;function i(){t<e.length&&(s.textContent=e.substring(0,t+1),t++,setTimeout(i,120))}setTimeout(i,500)}}document.addEventListener("DOMContentLoaded",updateTerminalPath),window.addEventListener("popstate",updateTerminalPath)</script><footer class=footer><div class=footer-content><div class=copyright>© 2025</div><div class=author>Sherif Ahmed</div></div></footer></body></html>