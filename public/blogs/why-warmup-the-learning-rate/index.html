<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Why Warmup the Learning Rate? | Sherif Ahmed</title><meta name=keywords content="Deep Learning,Optimizers"><meta name=description content="In this blog, I will try break down the findings from the paper ‚ÄúWhy Warmup the Learning Rate? Underlying Mechanisms and Improvements‚Äù and explain how warm-up helps stabilize training by reducing gradient sharpness and enabling the use of higher learning rates."><meta name=author content="Sherif Ahmed"><link rel=canonical href=http://localhost:1313/blogs/why-warmup-the-learning-rate/><link crossorigin=anonymous href=/assets/css/stylesheet.43675f0167f994c072fb1a6bd61d8502d9877943dd538a474f7d64fa346ba64c.css integrity="sha256-Q2dfAWf5lMBy+xpr1h2FAtmHeUPdU4pHT31k+jRrpkw=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/icon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blogs/why-warmup-the-learning-rate/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:1313/blogs/why-warmup-the-learning-rate/"><meta property="og:site_name" content="Sherif Ahmed"><meta property="og:title" content="Why Warmup the Learning Rate?"><meta property="og:description" content="In this blog, I will try break down the findings from the paper ‚ÄúWhy Warmup the Learning Rate? Underlying Mechanisms and Improvements‚Äù and explain how warm-up helps stabilize training by reducing gradient sharpness and enabling the use of higher learning rates."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-06-30T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-30T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Optimizers"><meta property="og:image" content="http://localhost:1313/blogs/warm-up/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/blogs/warm-up/cover.png"><meta name=twitter:title content="Why Warmup the Learning Rate?"><meta name=twitter:description content="In this blog, I will try break down the findings from the paper ‚ÄúWhy Warmup the Learning Rate? Underlying Mechanisms and Improvements‚Äù and explain how warm-up helps stabilize training by reducing gradient sharpness and enabling the use of higher learning rates."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blogs/"},{"@type":"ListItem","position":2,"name":"Why Warmup the Learning Rate?","item":"http://localhost:1313/blogs/why-warmup-the-learning-rate/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Why Warmup the Learning Rate?","name":"Why Warmup the Learning Rate?","description":"In this blog, I will try break down the findings from the paper ‚ÄúWhy Warmup the Learning Rate? Underlying Mechanisms and Improvements‚Äù and explain how warm-up helps stabilize training by reducing gradient sharpness and enabling the use of higher learning rates.","keywords":["Deep Learning","Optimizers"],"articleBody":"Why Do We Use Learning Rate Warm-Up in Deep Learning? Training deep neural networks is notoriously sensitive to hyperparameters, especially the learning rate. One widely adopted technique to improve stability and performance is learning rate warm-up. But why does warm-up help, what exactly does it do and the effect of Warm-up duration and how does it behave with different optimizers like SGD and Adam?\nWhat Is Learning Rate Warm-Up? Learning rate warm-up is a simple technique where the learning rate starts small and gradually increases to a target value over a few iterations or epochs.\nExample:\nIf your target learning rate is 1e-4, instead of starting training with 1e-4 immediately, you might increase the learning rate gradually smaller value (e.g., 1e-6) to 1e-4 over the first 5 epochs the first 5 epochs this is known as the warm-up period (or warm-up steps if measured in iterations).\nPyTorch Code Example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 import math import torch import torch.nn as nn import torch.optim as optim import matplotlib.pyplot as plt from torch.utils.data import DataLoader, TensorDataset from torch.optim.lr_scheduler import LambdaLR # Config base_lr = 1e-4 warmup_epochs = 5 total_epochs = 20 batch_size = 512 steps_per_epoch = 1000 // batch_size # dataset size / batch size total_steps = total_epochs * steps_per_epoch warmup_steps = warmup_epochs * steps_per_epoch # Dummy data and model X, y = torch.randn(1000, 10), torch.randint(0, 2, (1000,)) loader = DataLoader(TensorDataset(X, y), batch_size=batch_size, shuffle=True) model = nn.Sequential(nn.Linear(10, 64), nn.ReLU(), nn.Linear(64, 2)) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=base_lr) # Cosine warm-up scheduler def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps): \"\"\" Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer. \"\"\" def lr_lambda(current_step): if current_step \u003c num_warmup_steps: return float(current_step) / float(max(1, num_warmup_steps)) progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps)) return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress))) return LambdaLR(optimizer, lr_lambda) # ---- Scheduler ---- scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps) lrs = [] # Training loop for epoch in range(total_epochs): model.train() total_loss = 0 for xb, yb in loader: optimizer.zero_grad() loss = criterion(model(xb), yb) loss.backward() optimizer.step() total_loss += loss.item() scheduler.step() # Update learning rate based on current epoch lrs.append(optimizer.param_groups[0]['lr']) print(f\"Epoch {epoch+1:2d}, LR: {lrs[-1]:.4f}, Loss: {total_loss / len(loader):.4f}\") Visualize the Learning Rate Schedule\n‚ö†Ô∏è Problem Without Warm-Up: Unstable Gradients At the start of training, neural networks often have randomly initialized weights. Applying the gradient steps at the beginning of training are not meaningful, and thus it would be harmful to take large steps in such directions, especially when:\nThe gradients are sharp they change rapidly with respect to input. The loss landscape is highly curved in certain directions. This can cause exploding gradients, oscillations, or divergence.\nAlso training instabilities, often referred to as ‚Äòcatapults‚Äô arises when the learning rate $\\eta$ exceeds a critical threshold $\\eta_c$, i.e.,\n$$ \\eta \u003e \\eta_c(t) $$\nTwo behaviors follow:\nMild Overshoot: If $\\eta_c \u003c \\eta \u003c \\eta_{\\max}$, training becomes temporarily unstable but self-stabilizes during training. Severe Overshoot: If $\\eta \u003e \\eta_{\\max}$, training suffers catastrophic divergence, sometimes called a catapult effect. Warm-Up Reduces Gradient Sharpness What is Sharpness? Sharpness refers to how rapidly the loss changes in different directions in weight space. Mathematically, it‚Äôs related to the Maximum eigenvalue of the Hessian (second derivative matrix of the loss): :\n$$ \\lambda_H(t) := \\lambda_{\\text{max}}(\\nabla^2_\\theta \\mathcal{L}) $$\nSharp gradient = large Hessian eigenvalues High sharpness implies instability with large learning rates. Effect of Warm-Up (Warm-Up as a Stabilizer): Warm-up reduces gradient sharpness early in training. As a result, the network:\nLearns smoother, more stable gradients Becomes more robust to the final large learning rate after warm-up Avoids divergence in early epochs At that subplot (a), (d) loss decreases more smoothly (indicating more stable network).\nAt that subplot (c), (f) that $\\lambda^H$ (Largest eigenvalue of Hessian) are more smoother (indicating loss sharpness across lose curvature).\nLong warm-up (bigger $T_{wrm}$) delays aggressive learning, letting model ‚Äúadapt its curvature‚Äù first.\nWarm-up acts as a step that brings the network into a ‚Äúsafe zone‚Äù of the loss landscape where sharpness is lower and training is more stable.\nWarm-Up Allows Higher Learning Rates Learning rate grows slowly, avoiding premature crossing of $\\eta_c$. Gives network time to smooth the loss surface. prepares the network to tolerate a higher learning rate after the warm-up phase.\nIn Figure, the authors measure test loss across combinations of:\nTarget learning rate $\\eta_{\\text{trgt}}$ Warm-up duration $T_{\\text{warm}}$ Two optimizers are compared:\n(a) Adam (b) GI-Adam (gradient injection variant) Observations:\nWith short warm-up durations, high learning rates lead to catastrophic loss explosions (deep blue areas).\nLonger warm-ups shift the safe zone rightward ‚Üí allowing higher learning rates without divergence.\nThis suggest that warm-up extends the stability region in the $(\\eta_{\\text{trgt}}, T_{\\text{warm}})$ space.\nKey Finding: Longer warm-up leads to more reduction in sharpness, and therefore even larger learning rates can be used safely.\nWhy is this important? Large learning rates speed up training by taking bigger steps (less time - less compute). Less time speed for hyperparameter tuning. ‚öñÔ∏è Warm-Up in Adam vs. SGD: What‚Äôs the Difference? Warm-up benefits both SGD and Adam, but in different ways:\nWarm-Up + Adam: The adaptive optimizers such as Adam argues that the variance of the adaptive learning rate is large during early training because the network has seen too few training samples.\nWarmup acts as a variance reduction method by allowing the network to collect accurate statistics of the gradient moments before using larger learning rates.\nWarm-Up + SGD: SGD is more sensitive to sharpness because it lacks adaptive scaling. Warm-up has a strong effect on SGD‚Äôs stability and learning rate tolerance. Without warm-up, SGD can easily diverge at high learning rates. Both optimizers show improved stability and performance with warm-up, but the benefit is more critical for Adaptive optimizers.\nTL;DR - Warm-Up Effects in Training Aspect Effect of Warm-Up Gradient Sharpness Gradually reduces sharpness (Hessian maximum eigenvalue) early in training Stability Reduce possibility training divergence Learning Rate Tolerance Enables safe use of higher learning rates post-warm-up Warm-Up Duration More warm-up steps = more robustness, but slower initial progress SGD vs. Adam Beneficial for both, but more critical for Adam due to adaptive variance üìö References ‚ÄúUnderlying Mechanisms Behind Learning Rate Warmup‚Äôs Success‚Äù ‚ÄúA Theoretical Understanding of Learning Rate Warmup in Deep Learning‚Äù ","wordCount":"1115","inLanguage":"en","image":"http://localhost:1313/blogs/warm-up/cover.png","datePublished":"2025-06-30T00:00:00Z","dateModified":"2025-06-30T00:00:00Z","author":{"@type":"Person","name":"Sherif Ahmed"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blogs/why-warmup-the-learning-rate/"},"publisher":{"@type":"Organization","name":"Sherif Ahmed","logo":{"@type":"ImageObject","url":"http://localhost:1313/icon.ico"}}}</script></head><body class=dark id=top><!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Your Website Title</title><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(function(){})}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><header class=header style="position:sticky;top:0;z-index:100;background:var(--theme);box-shadow:0 2px 4px rgba(0,0,0,1%)"><nav class=nav><div class=logo><a href=http://localhost:1313/ class=header-terminal><span class=header-terminal-text></span>
<span class=header-terminal-cursor></span></a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:1313/experience/ title=Experience><span>Experience</span></a></li><li><a href=http://localhost:1313/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:1313/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header></body></html><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">Why Warmup the Learning Rate?</h1><div class=post-description>In this blog, I will try break down the findings from the paper ‚ÄúWhy Warmup the Learning Rate? Underlying Mechanisms and Improvements‚Äù and explain how warm-up helps stabilize training by reducing gradient sharpness and enabling the use of higher learning rates.</div><div class=post-meta><span title='2025-06-30 00:00:00 +0000 UTC'>June 30, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1115 words&nbsp;¬∑&nbsp;Sherif Ahmed</div></header><figure class=entry-cover><img loading=eager src=http://localhost:1313/blogs/warm-up/cover.png alt="Why Warmup the Learning Rate?"></figure><div class=post-content><h1 id=why-do-we-use-learning-rate-warm-up-in-deep-learning>Why Do We Use Learning Rate Warm-Up in Deep Learning?<a hidden class=anchor aria-hidden=true href=#why-do-we-use-learning-rate-warm-up-in-deep-learning>#</a></h1><p>Training deep neural networks is notoriously sensitive to hyperparameters, especially the learning rate. One widely adopted technique to improve stability and performance is <strong>learning rate warm-up</strong>. But <strong>why</strong> does warm-up help, <strong>what exactly does it do</strong> and <strong>the effect of Warm-up duration</strong> and <strong>how does it behave with different optimizers</strong> like SGD and Adam?</p><h2 id=what-is-learning-rate-warm-up>What Is Learning Rate Warm-Up?<a hidden class=anchor aria-hidden=true href=#what-is-learning-rate-warm-up>#</a></h2><p>Learning rate warm-up is a simple technique where the learning rate starts small and gradually increases to a target value over a few iterations or epochs.</p><p><strong>Example:</strong></p><p>If your target learning rate is 1e-4, instead of starting training with 1e-4 immediately, you might increase the learning rate gradually smaller value (e.g., 1e-6) to 1e-4 over the first 5 epochs the first 5 epochs this is known as the warm-up period (or warm-up steps if measured in iterations).</p><p>PyTorch Code Example</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.optim</span> <span class=k>as</span> <span class=nn>optim</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.utils.data</span> <span class=kn>import</span> <span class=n>DataLoader</span><span class=p>,</span> <span class=n>TensorDataset</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.optim.lr_scheduler</span> <span class=kn>import</span> <span class=n>LambdaLR</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Config</span>
</span></span><span class=line><span class=cl><span class=n>base_lr</span> <span class=o>=</span> <span class=mf>1e-4</span>
</span></span><span class=line><span class=cl><span class=n>warmup_epochs</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>total_epochs</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=n>steps_per_epoch</span> <span class=o>=</span> <span class=mi>1000</span> <span class=o>//</span> <span class=n>batch_size</span>  <span class=c1># dataset size / batch size</span>
</span></span><span class=line><span class=cl><span class=n>total_steps</span> <span class=o>=</span> <span class=n>total_epochs</span> <span class=o>*</span> <span class=n>steps_per_epoch</span>
</span></span><span class=line><span class=cl><span class=n>warmup_steps</span> <span class=o>=</span> <span class=n>warmup_epochs</span> <span class=o>*</span> <span class=n>steps_per_epoch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Dummy data and model</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1000</span><span class=p>,</span> <span class=mi>10</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=p>(</span><span class=mi>1000</span><span class=p>,))</span>
</span></span><span class=line><span class=cl><span class=n>loader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span><span class=n>TensorDataset</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>),</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>64</span><span class=p>),</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>base_lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cosine warm-up scheduler</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_cosine_schedule_with_warmup</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=p>,</span> <span class=n>num_training_steps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Create a schedule with a learning rate that decreases following the values of the cosine function
</span></span></span><span class=line><span class=cl><span class=s2>        between the initial lr set in the optimizer to 0, after a warmup period during which it increases
</span></span></span><span class=line><span class=cl><span class=s2>        linearly between 0 and the initial lr set in the optimizer.
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>lr_lambda</span><span class=p>(</span><span class=n>current_step</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>current_step</span> <span class=o>&lt;</span> <span class=n>num_warmup_steps</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=nb>float</span><span class=p>(</span><span class=n>current_step</span><span class=p>)</span> <span class=o>/</span> <span class=nb>float</span><span class=p>(</span><span class=nb>max</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>progress</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=n>current_step</span> <span class=o>-</span> <span class=n>num_warmup_steps</span><span class=p>)</span> <span class=o>/</span> <span class=nb>float</span><span class=p>(</span><span class=nb>max</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_training_steps</span> <span class=o>-</span> <span class=n>num_warmup_steps</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=nb>max</span><span class=p>(</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=mf>1.0</span> <span class=o>+</span> <span class=n>math</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>pi</span> <span class=o>*</span> <span class=n>progress</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>LambdaLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>lr_lambda</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ---- Scheduler ----</span>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>get_cosine_schedule_with_warmup</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>num_warmup_steps</span><span class=o>=</span><span class=n>warmup_steps</span><span class=p>,</span> <span class=n>num_training_steps</span><span class=o>=</span><span class=n>total_steps</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>lrs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Training loop</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>total_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>total_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>xb</span><span class=p>,</span> <span class=n>yb</span> <span class=ow>in</span> <span class=n>loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>model</span><span class=p>(</span><span class=n>xb</span><span class=p>),</span> <span class=n>yb</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>total_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>  <span class=c1># Update learning rate based on current epoch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>lrs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>optimizer</span><span class=o>.</span><span class=n>param_groups</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;lr&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>:</span><span class=s2>2d</span><span class=si>}</span><span class=s2>, LR: </span><span class=si>{</span><span class=n>lrs</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, Loss: </span><span class=si>{</span><span class=n>total_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>loader</span><span class=p>)</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Visualize the Learning Rate Schedule</p><p><img alt="Visualize Cosine Warm-Up LR Schedule" loading=lazy src=/blogs/warm-up/Cosine%20Warm-Up%20LR%20Schedule.png.png></p><hr><h2 id=-problem-without-warm-up-unstable-gradients>‚ö†Ô∏è Problem Without Warm-Up: Unstable Gradients<a hidden class=anchor aria-hidden=true href=#-problem-without-warm-up-unstable-gradients>#</a></h2><p>At the start of training, neural networks often have <strong>randomly initialized weights</strong>. Applying the gradient steps at the
beginning of training are not meaningful, and thus it would be harmful to take large steps in such
directions, especially when:</p><ul><li>The gradients are <strong>sharp</strong> they change rapidly with respect to input.</li><li>The loss landscape is highly curved in certain directions.</li></ul><p>This can cause <strong>exploding gradients</strong>, <strong>oscillations</strong>, or <strong>divergence</strong>.</p><p>Also training instabilities, often referred to as ‚Äòcatapults‚Äô arises when the learning rate $\eta$ exceeds a critical threshold $\eta_c$, i.e.,</p><p>$$
\eta > \eta_c(t)
$$</p><p>Two behaviors follow:</p><ul><li><strong>Mild Overshoot:</strong> If $\eta_c &lt; \eta &lt; \eta_{\max}$, training becomes temporarily unstable but <strong>self-stabilizes</strong> during training.</li><li><strong>Severe Overshoot:</strong> If $\eta > \eta_{\max}$, training suffers <strong>catastrophic divergence</strong>, sometimes called a <strong>catapult effect</strong>.</li></ul><hr><h2 id=warm-up-reduces-gradient-sharpness>Warm-Up Reduces Gradient Sharpness<a hidden class=anchor aria-hidden=true href=#warm-up-reduces-gradient-sharpness>#</a></h2><h3 id=what-is-sharpness>What is Sharpness?<a hidden class=anchor aria-hidden=true href=#what-is-sharpness>#</a></h3><p>Sharpness refers to how rapidly the loss changes in different directions in weight space. Mathematically, it&rsquo;s related to the <strong>Maximum eigenvalue of the Hessian</strong> (second derivative matrix of the loss): :</p><p>$$
\lambda_H(t) := \lambda_{\text{max}}(\nabla^2_\theta \mathcal{L})
$$</p><ul><li><strong>Sharp gradient = large Hessian eigenvalues</strong></li><li>High sharpness implies instability with large learning rates.</li></ul><hr><h3 id=effect-of-warm-up--warm-up-as-a-stabilizer>Effect of Warm-Up (Warm-Up as a Stabilizer):<a hidden class=anchor aria-hidden=true href=#effect-of-warm-up--warm-up-as-a-stabilizer>#</a></h3><p>Warm-up <strong>reduces gradient sharpness</strong> early in training. As a result, the network:</p><ul><li>Learns smoother, more stable gradients</li><li>Becomes more <strong>robust</strong> to the final large learning rate after warm-up</li><li>Avoids <strong>divergence</strong> in early epochs</li></ul><p><img loading=lazy src=/blogs/warm-up/figure_1.png></p><ul><li><p>At that subplot (a), (d) loss decreases more smoothly (indicating more stable network).</p></li><li><p>At that subplot (c), (f) that $\lambda^H$ (Largest eigenvalue of Hessian) are more smoother (indicating loss sharpness across lose curvature).</p></li><li><p>Long warm-up (bigger $T_{wrm}$) delays aggressive learning, letting model ‚Äúadapt its curvature‚Äù first.</p></li></ul><blockquote><p>Warm-up acts as a step that brings the network into a &ldquo;safe zone&rdquo; of the loss landscape where sharpness is lower and training is more stable.</p></blockquote><hr><h2 id=warm-up-allows-higher-learning-rates>Warm-Up Allows Higher Learning Rates<a hidden class=anchor aria-hidden=true href=#warm-up-allows-higher-learning-rates>#</a></h2><p>Learning rate grows slowly, avoiding premature crossing of $\eta_c$. Gives network time to smooth the loss surface. prepares the network to <strong>tolerate a higher learning rate</strong> after the warm-up phase.</p><p><img loading=lazy src=/blogs/warm-up/figure_2.png></p><p>In Figure, the authors measure <strong>test loss</strong> across combinations of:</p><ul><li><strong>Target learning rate</strong> $\eta_{\text{trgt}}$</li><li><strong>Warm-up duration</strong> $T_{\text{warm}}$</li></ul><p>Two optimizers are compared:</p><ul><li><strong>(a) Adam</strong></li><li><strong>(b) GI-Adam</strong> (gradient injection variant)</li></ul><p>Observations:</p><ul><li><p>With <strong>short warm-up durations</strong>, high learning rates lead to <strong>catastrophic loss explosions</strong> (deep blue areas).</p></li><li><p><strong>Longer warm-ups</strong> shift the safe zone rightward ‚Üí allowing <strong>higher learning rates</strong> without divergence.</p></li><li><p>This suggest that <strong>warm-up extends the stability region</strong> in the $(\eta_{\text{trgt}}, T_{\text{warm}})$ space.</p></li></ul><h3 id=key-finding>Key Finding:<a hidden class=anchor aria-hidden=true href=#key-finding>#</a></h3><blockquote><p>Longer warm-up leads to more reduction in sharpness, and therefore even <strong>larger learning rates can be used safely</strong>.</p></blockquote><h3 id=why-is-this-important>Why is this important?<a hidden class=anchor aria-hidden=true href=#why-is-this-important>#</a></h3><ul><li><strong>Large learning rates</strong> speed up training by taking bigger steps (less time - less compute).</li><li>Less time speed for hyperparameter tuning.</li></ul><hr><h2 id=-warm-up-in-adam-vs-sgd-whats-the-difference>‚öñÔ∏è Warm-Up in Adam vs. SGD: What&rsquo;s the Difference?<a hidden class=anchor aria-hidden=true href=#-warm-up-in-adam-vs-sgd-whats-the-difference>#</a></h2><p>Warm-up benefits both <strong>SGD</strong> and <strong>Adam</strong>, but in different ways:</p><h3 id=warm-up--adam>Warm-Up + Adam:<a hidden class=anchor aria-hidden=true href=#warm-up--adam>#</a></h3><ul><li><p>The adaptive optimizers such as Adam argues that the variance of the adaptive learning rate is large during early training
because the network has seen too few training samples.</p></li><li><p>Warmup acts as a variance reduction method by allowing the network to collect accurate statistics of the gradient moments before using larger learning rates.</p></li></ul><h3 id=warm-up--sgd>Warm-Up + SGD:<a hidden class=anchor aria-hidden=true href=#warm-up--sgd>#</a></h3><ul><li>SGD is <strong>more sensitive to sharpness</strong> because it lacks adaptive scaling.</li><li>Warm-up has a <strong>strong effect</strong> on SGD‚Äôs stability and learning rate tolerance.</li><li>Without warm-up, SGD can easily diverge at high learning rates.</li></ul><blockquote><p>Both optimizers show improved stability and performance with warm-up, but the <strong>benefit is more critical for Adaptive optimizers</strong>.</p></blockquote><hr><h3 id=tldr---warm-up-effects-in-training>TL;DR - Warm-Up Effects in Training<a hidden class=anchor aria-hidden=true href=#tldr---warm-up-effects-in-training>#</a></h3><table><thead><tr><th><strong>Aspect</strong></th><th><strong>Effect of Warm-Up</strong></th></tr></thead><tbody><tr><td><strong>Gradient Sharpness</strong></td><td>Gradually reduces sharpness (Hessian maximum eigenvalue) early in training</td></tr><tr><td><strong>Stability</strong></td><td>Reduce possibility training divergence</td></tr><tr><td><strong>Learning Rate Tolerance</strong></td><td>Enables safe use of higher learning rates post-warm-up</td></tr><tr><td><strong>Warm-Up Duration</strong></td><td>More warm-up steps = more robustness, but slower initial progress</td></tr><tr><td><strong>SGD vs. Adam</strong></td><td>Beneficial for both, but <strong>more critical for Adam</strong> due to adaptive variance</td></tr><tr><td></td><td></td></tr></tbody></table><hr><h2 id=-references>üìö References<a hidden class=anchor aria-hidden=true href=#-references>#</a></h2><ul><li><a href="https://youtu.be/KOKvSQQJYy4?si=dLad0VPGvoT0pD4O">&ldquo;Underlying Mechanisms Behind Learning Rate Warmup&rsquo;s Success&rdquo;</a></li><li><a href=https://arxiv.org/pdf/2406.09405v1>&ldquo;A Theoretical Understanding of Learning Rate Warmup in Deep Learning&rdquo;</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/deep-learning/>Deep Learning</a></li><li><a href=http://localhost:1313/tags/optimizers/>Optimizers</a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=http://localhost:1313/js/experience-timeline.js></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><style>.terminal-line-container{display:flex;justify-content:center;margin-top:5rem;margin-bottom:2rem}.terminal-line{font-family:monospace;background-color:#212121;padding:15px;border-radius:5px;color:#fafafa;display:flex;align-items:center;min-width:300px}.terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:#fafafa;animation:blink 1s step-end infinite;margin-left:4px}@keyframes blink{from,to{background-color:initial}50%{background-color:#fafafa}}.header-terminal{font-family:courier new,Courier,monospace;padding:6px 12px;color:rgba(255,255,255,.9);display:flex;align-items:center;font-size:.95rem;margin-right:16px;text-decoration:none;transition:all .2s ease;letter-spacing:-.5px}.header-terminal:hover{transform:translateY(-1px);opacity:.8}.header-terminal-text{white-space:nowrap;font-weight:600;color:rgba(255,255,255,.9)}.header-terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:rgba(255,255,255,.9);animation:blink .8s step-end infinite;margin-left:4px;opacity:.9}@media(max-width:600px){.header-terminal{font-size:.85rem;padding:4px 8px}}.footer-content{display:flex;flex-direction:row;align-items:center;justify-content:center;padding:0rem 1;gap:.5rem;text-align:center}.copyright{color:var(--secondary);font-size:.9rem}.author{color:var(--primary);font-size:1rem;font-weight:500}.entry-content .profile-mode-image img{border-radius:50%;object-fit:cover;aspect-ratio:1;border:3px solid var(--primary);transition:transform .3s ease,border-color .3s ease}.entry-content .profile-mode-image img:hover{transform:scale(1.05);border-color:var(--secondary)}</style><script>(function(){const t="cd /home/about",n=document.getElementById("terminal-text");if(!n)return;let e=0;function s(){if(e<t.length)n.textContent+=t.charAt(e),e++,setTimeout(s,120);else{const e=document.querySelector(".terminal-cursor");e&&(e.style.animation="none",e.style.backgroundColor="#fafafa")}}const o=new IntersectionObserver(e=>{e.forEach(e=>{e.isIntersecting&&(setTimeout(s,500),o.unobserve(e.target))})}),i=document.querySelector(".terminal-line");i&&o.observe(i)})();function updateTerminalPath(){const t=window.location.pathname;let e="> cd";t==="/"?e+=" /home":t.endsWith("/")?e+=" /home"+t.slice(0,-1):e+=" /home"+t;const n=document.querySelector(".header-terminal-text");if(n){let t=0;n.textContent="> ";function o(){t<e.length&&(n.textContent=e.substring(0,t+1),t++,setTimeout(o,35))}o()}const s=document.getElementById("terminal-text");if(s){s.textContent="";let t=0;function i(){t<e.length&&(s.textContent=e.substring(0,t+1),t++,setTimeout(i,120))}setTimeout(i,500)}}document.addEventListener("DOMContentLoaded",updateTerminalPath),window.addEventListener("popstate",updateTerminalPath)</script><footer class=footer><div class=footer-content><div class=copyright>¬© 2025</div><div class=author>Sherif Ahmed</div></div></footer></body></html>