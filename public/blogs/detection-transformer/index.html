<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=38883&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DETR End-to-End Object Detection with Transformers | Sherif Ahmed</title><meta name=keywords content="Deep Learning,Computer Vision,Object Detection,Transformers"><meta name=description content="The Detection Transformer (DETR) represents a novel, end-to-end approach to object detection, reframing the task as a set prediction problem. This architecture relies on a transformer encoder-decoder structure and a unique assignment strategy involving the Hungarian matching algorithm, enabling the model to bypass post-processing steps like Non-Maximal Suppression (NMS) and reliance on anchor design."><meta name=author content="Sherif Ahmed"><link rel=canonical href=http://localhost:38883/blogs/detection-transformer/><link crossorigin=anonymous href=/assets/css/stylesheet.43675f0167f994c072fb1a6bd61d8502d9877943dd538a474f7d64fa346ba64c.css integrity="sha256-Q2dfAWf5lMBy+xpr1h2FAtmHeUPdU4pHT31k+jRrpkw=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:38883/icon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:38883/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:38883/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:38883/apple-touch-icon.png><link rel=mask-icon href=http://localhost:38883/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:38883/blogs/detection-transformer/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="http://localhost:38883/blogs/detection-transformer/"><meta property="og:site_name" content="Sherif Ahmed"><meta property="og:title" content="DETR End-to-End Object Detection with Transformers"><meta property="og:description" content="The Detection Transformer (DETR) represents a novel, end-to-end approach to object detection, reframing the task as a set prediction problem. This architecture relies on a transformer encoder-decoder structure and a unique assignment strategy involving the Hungarian matching algorithm, enabling the model to bypass post-processing steps like Non-Maximal Suppression (NMS) and reliance on anchor design."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blogs"><meta property="article:published_time" content="2025-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-26T00:00:00+00:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Object-Detection"><meta property="article:tag" content="Transformers"><meta property="og:image" content="http://localhost:38883/blogs/detr/image14.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:38883/blogs/detr/image14.png"><meta name=twitter:title content="DETR End-to-End Object Detection with Transformers"><meta name=twitter:description content="The Detection Transformer (DETR) represents a novel, end-to-end approach to object detection, reframing the task as a set prediction problem. This architecture relies on a transformer encoder-decoder structure and a unique assignment strategy involving the Hungarian matching algorithm, enabling the model to bypass post-processing steps like Non-Maximal Suppression (NMS) and reliance on anchor design."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:38883/blogs/"},{"@type":"ListItem","position":2,"name":"DETR End-to-End Object Detection with Transformers","item":"http://localhost:38883/blogs/detection-transformer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DETR End-to-End Object Detection with Transformers","name":"DETR End-to-End Object Detection with Transformers","description":"The Detection Transformer (DETR) represents a novel, end-to-end approach to object detection, reframing the task as a set prediction problem. This architecture relies on a transformer encoder-decoder structure and a unique assignment strategy involving the Hungarian matching algorithm, enabling the model to bypass post-processing steps like Non-Maximal Suppression (NMS) and reliance on anchor design.","keywords":["Deep Learning","Computer Vision","Object Detection","Transformers"],"articleBody":"DETR Architecture Components Backbone for Feature Extraction The initial input image is processed by a backbone, typically a pre-trained Convolutional Neural Network (CNN), such as ResNet 50 or ResNet 101, trained on the ImageNet classification task.\nThe last pooling and classification layers are discarded to produce a feature map that captures semantic information for different regions of the image.\nThe network stride is typically 32. The feature map output dimensions are $C \\times \\text{feature map height} \\times \\text{feature map width}$, where $C$ is the number of output channels of the last convolution layer.\nA projection layer, implemented as a $1 \\times 1$ convolution, is applied to transform the feature map dimensions. This step aligns the input channels with the backbone output and the output channels with the hidden dimension of the transformer, $D_{\\text{model}}$. The resulting shape is $D_{\\text{model}} \\times \\text{feature map height} \\times \\text{feature map width}$.\nTransformer Encoder The projected feature map is flattened by collapsing the spatial dimensions, creating a sequence of $D$-dimensional features that serves as input to the encoder.\nThe encoder is a stack of transformer encoder layers, each consisting of self-attention and feed-forward layers, integrated with residual connections and normalization steps.\nThrough self-attention, the encoder transforms the backbone features into representations conducive for the detection task, establishing relationships between distinct image parts and baking in contextual knowledge. Positional Encoding Due to the permutation invariant nature of transformers, spatial positional information must be injected.\nThe original transformer’s sinusoidal position encoding is adapted for 2D images.\nThe D-dimensional position encoding for a spatial feature is created by concatenating $D/2$ dimensional encodings for the height coordinates and $D/2$ dimensional encodings for the width coordinates.\nUnlike typical transformer usage, this positional information is added at each self-attention layer, specifically to the query ($Q$) and key ($K$) tensors, rather than just once at the input.\nTransformer Decoder and Object Queries The decoder requires two inputs: the transformed image features returned by the encoder, and $N$ object queries. Object queries are $N$ randomly initialized, learnable embeddings that act as “slots” or containers which the model transforms to produce object predictions. The number of predictions equals the number of queries.\nThe decoder is a sequence of layers containing self-attention, cross-attention, MLP, normalization, and residual connections.\nSelf-Attention in Decoder: Enables queries to interact with each other, allowing the model to reason about all objects together using pair-wise relationships.\nThe $N$ slots are typically initialized to zero, and $N$ embedding values, termed output position encoding, are added to the slots prior to computing $Q$ and $K$ representations within the attention mechanism. Cross-Attention: Allows object queries access to the entire image context by attending to the features returned by the encoder. The object queries form the $Q$ input, while the encoder image features form the $K$ and $V$ inputs. Positional information is added at each cross-attention layer: fixed sinusoidal positional embeddings are added to image features (prior to $K$ computation), and output position embeddings are added to slot representations (prior to $Q$ computation). Prediction Heads: The $N$ output vectors from the final decoder layer are decoded independently into $N$ object predictions using two parallel MLPs whose parameters are shared across all query slots. A class MLP outputs class probabilities for the predicted box. A bounding box MLP outputs four normalized coordinates ($c_x, c_y, w, h$). Set Prediction and Optimal Assignment DETR enforces a one-to-one mapping between predicted boxes and ground truth target boxes, ensuring that each target is assigned to a single prediction and each prediction is assigned to a single target or background.\nHungarian Matching Algorithm The Hungarian algorithm is employed to find the unique assignment that minimizes the total cost across all assignments for a given image.\nThis procedure is analogous to finding the minimum cost assignment of $N$ workers to $N$ tasks, requiring a square cost matrix.\nImplementation Strategy: To ensure a square matrix, the number of object queries is set to the maximum expected objects in any image (e.g., 100). If targets are fewer than queries, dummy background boxes are conceptually added to the target set; assigning a prediction to a dummy box incurs zero cost. Principle: The core mechanism relies on subtracting or adding a fixed number to all elements in a row or column, which modifies the costs but does not alter the identity of the optimal assignment. The goal is to maximize the number of zero-cost assignments.\nAssignment Cost Formulation The cost ($\\mathcal{C}$) of assigning a predicted box ($p$) to a target box ($t$) is a weighted sum of two primary components: classification cost and localization cost.\nClassification Cost: Based on the predicted probability score for the target box’s class label. Since low cost is desired when the probability is high, the cost is calculated as $1 - P_t$, where $P_t$ is the probability predicted for the target class $t$.\nLocalization Cost: This composite cost measures the closeness of the predicted box to the target box.\nL1 Distance: Computed between the normalized coordinates ($c_x, c_y, w, h$) of the target and predicted boxes.\nGeneralized IOU (GIOU): Used because L1 distance is sensitive to box size differences. To ensure lower cost corresponds to higher overlap, negative Generalized IOU is used.\nThe total cost for assignment $i$ is:\n$$ C_i = \\lambda_{cls} \\cdot Cost_{cls} + \\lambda_{L1} \\cdot Cost_{L1} + \\lambda_{GIOU} \\cdot Cost_{GIOU} $$\nDETR Set Prediction Training Loss The loss function utilizes the optimal assignment returned by the Hungarian algorithm to drive training. The loss is a weighted sum of classification loss and localization loss.\nClassification Loss Cross-Entropy Loss is applied between the target class labels and the predicted class probabilities for every box. For predicted boxes assigned to a ground truth target, the target label is the assigned target’s class. For unassigned boxes, the target label is background. Localization Loss Localization loss is computed only for predicted boxes assigned to a non-background class.\nThe components are:\nSmooth L1 Loss: Applied between the target box coordinates and the predicted box coordinates. Generalized IOU Loss (GIOU Loss): Defined as $1 - \\text{GIOU}$. Minimizing this loss encourages the model to increase the overlap between the predicted and matched target boxes during training. Auxiliary Losses To enhance convergence and improve performance, auxiliary losses are incorporated during training.\nOutputs from all decoder layers are utilized, with each layer’s output being passed through the shared class and bounding box MLPs to generate predictions.\nA separate Hungarian matching is performed for the set of predictions generated at each decoder layer.\nClassification and localization losses are computed for each layer based on these assignments.\nThe total loss is the summation of the losses from all decoder layers.\nThis approach ensures that earlier decoder layers learn useful representations, which the subsequent layers refine. Note that during inference, only the final decoder layer output is used.\n📚 References “DETR Explained | End-to-End Object Detection with Transformers | DETR Tutorial Part 1” “DETR PyTorch Implementation | DETR Tutorial Part 2” ","wordCount":"1149","inLanguage":"en","image":"http://localhost:38883/blogs/detr/image14.png","datePublished":"2025-09-26T00:00:00Z","dateModified":"2025-09-26T00:00:00Z","author":{"@type":"Person","name":"Sherif Ahmed"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:38883/blogs/detection-transformer/"},"publisher":{"@type":"Organization","name":"Sherif Ahmed","logo":{"@type":"ImageObject","url":"http://localhost:38883/icon.ico"}}}</script></head><body class=dark id=top><!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Your Website Title</title><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]},startup:{pageReady:()=>MathJax.startup.defaultPageReady().then(function(){})}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body><header class=header style="position:sticky;top:0;z-index:100;background:var(--theme);box-shadow:0 2px 4px rgba(0,0,0,1%)"><nav class=nav><div class=logo><a href=http://localhost:38883/ class=header-terminal><span class=header-terminal-text></span>
<span class=header-terminal-cursor></span></a><div class=logo-switches></div></div><ul id=menu><li><a href=http://localhost:38883/ title=Home><span>Home</span></a></li><li><a href=http://localhost:38883/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:38883/experience/ title=Experience><span>Experience</span></a></li><li><a href=http://localhost:38883/blogs/ title=Blogs><span>Blogs</span></a></li><li><a href=http://localhost:38883/tags/ title=Tags><span>Tags</span></a></li><li><a href=http://localhost:38883/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header></body></html><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:38883/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:38883/blogs/>Blogs</a></div><h1 class="post-title entry-hint-parent">DETR End-to-End Object Detection with Transformers</h1><div class=post-description>The Detection Transformer (DETR) represents a novel, end-to-end approach to object detection, reframing the task as a set prediction problem. This architecture relies on a transformer encoder-decoder structure and a unique assignment strategy involving the Hungarian matching algorithm, enabling the model to bypass post-processing steps like Non-Maximal Suppression (NMS) and reliance on anchor design.</div><div class=post-meta><span title='2025-09-26 00:00:00 +0000 UTC'>September 26, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1149 words&nbsp;·&nbsp;Sherif Ahmed</div></header><figure class=entry-cover><img loading=eager src=http://localhost:38883/blogs/detr/image14.png alt="Detection Transformer"></figure><div class=post-content><h3 id=detr-architecture-components>DETR Architecture Components<a hidden class=anchor aria-hidden=true href=#detr-architecture-components>#</a></h3><h4 id=backbone-for-feature-extraction>Backbone for Feature Extraction <img loading=lazy src=/blogs/detr/image1.png><a hidden class=anchor aria-hidden=true href=#backbone-for-feature-extraction>#</a></h4><ul><li><p>The initial input image is processed by a backbone, typically a pre-trained Convolutional Neural Network (CNN), such as <strong>ResNet 50 or ResNet 101</strong>, trained on the ImageNet classification task.</p></li><li><p>The last pooling and classification layers are discarded to produce a feature map that captures semantic information for different regions of the image.</p></li><li><p>The network stride is typically 32. The feature map output dimensions are $C \times \text{feature map height} \times \text{feature map width}$, where $C$ is the number of output channels of the last convolution layer.</p></li><li><p>A <strong>projection layer</strong>, implemented as a $1 \times 1$ convolution, is applied to transform the feature map dimensions. This step aligns the input channels with the backbone output and the output channels with the hidden dimension of the transformer, $D_{\text{model}}$. The resulting shape is $D_{\text{model}} \times \text{feature map height} \times \text{feature map width}$.</p></li></ul><h4 id=transformer-encoder>Transformer Encoder <img loading=lazy src=/blogs/detr/image2.png><a hidden class=anchor aria-hidden=true href=#transformer-encoder>#</a></h4><ul><li><p>The projected feature map is flattened by collapsing the spatial dimensions, creating a sequence of $D$-dimensional features that serves as input to the encoder.</p></li><li><p>The encoder is a stack of transformer encoder layers, each consisting of self-attention and feed-forward layers, integrated with residual connections and normalization steps.</p></li><li><p>Through self-attention, the encoder transforms the backbone features into representations conducive for the detection task, establishing relationships between distinct image parts and baking in contextual knowledge. <img loading=lazy src=/blogs/detr/image3.png></p></li></ul><h5 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h5><ul><li><p>Due to the permutation invariant nature of transformers, <strong>spatial positional information</strong> must be injected.</p></li><li><p>The original transformer&rsquo;s sinusoidal position encoding is adapted for 2D images.</p></li><li><p>The D-dimensional position encoding for a spatial feature is created by concatenating $D/2$ dimensional encodings for the height coordinates and $D/2$ dimensional encodings for the width coordinates.</p></li><li><p>Unlike typical transformer usage, this positional information is added at <strong>each self-attention layer</strong>, specifically to the query ($Q$) and key ($K$) tensors, rather than just once at the input.</p></li></ul><h4 id=transformer-decoder-and-object-queries>Transformer Decoder and Object Queries<a hidden class=anchor aria-hidden=true href=#transformer-decoder-and-object-queries>#</a></h4><ul><li><p>The decoder requires two inputs: the transformed image features returned by the encoder, and <strong>$N$ object queries</strong>. <img loading=lazy src=/blogs/detr/image4.png></p></li><li><p>Object queries are $N$ randomly initialized, learnable embeddings that act as &ldquo;slots&rdquo; or containers which the model transforms to produce object predictions. The number of predictions equals the number of queries.</p></li><li><p>The decoder is a sequence of layers containing self-attention, cross-attention, MLP, normalization, and residual connections.</p></li><li><p><strong>Self-Attention in Decoder:</strong> Enables queries to interact with each other, allowing the model to reason about all objects together using pair-wise relationships.</p><ul><li>The $N$ slots are typically initialized to zero, and $N$ embedding values, termed <strong>output position encoding</strong>, are added to the slots prior to computing $Q$ and $K$ representations within the attention mechanism. <img loading=lazy src=/blogs/detr/image5.png></li></ul></li><li><p><strong>Cross-Attention:</strong> Allows object queries access to the entire image context by attending to the features returned by the encoder. <img loading=lazy src=/blogs/detr/image6.png></p><ul><li>The object queries form the $Q$ input, while the encoder image features form the $K$ and $V$ inputs.</li><li>Positional information is added at each cross-attention layer: fixed sinusoidal positional embeddings are added to image features (prior to $K$ computation), and output position embeddings are added to slot representations (prior to $Q$ computation). <img loading=lazy src=/blogs/detr/image7.png></li></ul></li><li><p><strong>Prediction Heads:</strong> The $N$ output vectors from the final decoder layer are decoded independently into $N$ object predictions using two parallel MLPs whose parameters are shared across all query slots. <img loading=lazy src=/blogs/detr/image8.png></p><ul><li>A <strong>class MLP</strong> outputs class probabilities for the predicted box.</li><li>A <strong>bounding box MLP</strong> outputs four normalized coordinates ($c_x, c_y, w, h$).</li></ul></li></ul><h3 id=set-prediction-and-optimal-assignment>Set Prediction and Optimal Assignment<a hidden class=anchor aria-hidden=true href=#set-prediction-and-optimal-assignment>#</a></h3><p>DETR enforces a one-to-one mapping between predicted boxes and ground truth target boxes, ensuring that each target is assigned to a single prediction and each prediction is assigned to a single target or background.</p><h4 id=hungarian-matching-algorithm>Hungarian Matching Algorithm<a hidden class=anchor aria-hidden=true href=#hungarian-matching-algorithm>#</a></h4><ul><li><p>The <strong>Hungarian algorithm</strong> is employed to find the unique assignment that minimizes the total cost across all assignments for a given image.</p></li><li><p>This procedure is analogous to finding the minimum cost assignment of $N$ workers to $N$ tasks, requiring a square cost matrix.<img loading=lazy src=/blogs/detr/image9.png></p></li><li><p><strong>Implementation Strategy:</strong> To ensure a square matrix, the number of object queries is set to the maximum expected objects in any image (e.g., 100). If targets are fewer than queries, dummy background boxes are conceptually added to the target set; assigning a prediction to a dummy box incurs zero cost.
<img loading=lazy src=/blogs/detr/image10.png></p></li><li><p><strong>Principle:</strong> The core mechanism relies on subtracting or adding a fixed number to all elements in a row or column, which modifies the costs but does not alter the identity of the optimal assignment. The goal is to maximize the number of zero-cost assignments.<img loading=lazy src=/blogs/detr/image11.png><img loading=lazy src=/blogs/detr/image12.png><img loading=lazy src=/blogs/detr/image13.png></p></li></ul><h4 id=assignment-cost-formulation>Assignment Cost Formulation <img loading=lazy src=/blogs/detr/image14.png><a hidden class=anchor aria-hidden=true href=#assignment-cost-formulation>#</a></h4><p>The cost ($\mathcal{C}$) of assigning a predicted box ($p$) to a target box ($t$) is a weighted sum of two primary components: classification cost and localization cost.</p><ol><li><p><strong>Classification Cost:</strong> Based on the predicted probability score for the target box&rsquo;s class label. Since low cost is desired when the probability is high, the cost is calculated as $1 - P_t$, where $P_t$ is the probability predicted for the target class $t$.</p></li><li><p><strong>Localization Cost:</strong> This composite cost measures the closeness of the predicted box to the target box.</p><ul><li><p><strong>L1 Distance:</strong> Computed between the normalized coordinates ($c_x, c_y, w, h$) of the target and predicted boxes.</p></li><li><p><strong>Generalized IOU (GIOU):</strong> Used because L1 distance is sensitive to box size differences. To ensure lower cost corresponds to higher overlap, negative Generalized IOU is used.</p></li></ul></li></ol><p>The total cost for assignment $i$ is:</p><p>$$
C_i = \lambda_{cls} \cdot Cost_{cls} + \lambda_{L1} \cdot Cost_{L1} + \lambda_{GIOU} \cdot Cost_{GIOU}
$$</p><h3 id=detr-set-prediction-training-loss>DETR Set Prediction Training Loss <img loading=lazy src=/blogs/detr/image15.png><a hidden class=anchor aria-hidden=true href=#detr-set-prediction-training-loss>#</a></h3><p>The loss function utilizes the optimal assignment returned by the Hungarian algorithm to drive training. The loss is a weighted sum of classification loss and localization loss.</p><h4 id=classification-loss>Classification Loss<a hidden class=anchor aria-hidden=true href=#classification-loss>#</a></h4><ul><li><strong>Cross-Entropy Loss</strong> is applied between the target class labels and the predicted class probabilities for every box.</li><li>For predicted boxes assigned to a ground truth target, the target label is the assigned target&rsquo;s class.</li><li>For unassigned boxes, the target label is <strong>background</strong>.<img loading=lazy src=/blogs/detr/image16.png></li></ul><h4 id=localization-loss>Localization Loss<a hidden class=anchor aria-hidden=true href=#localization-loss>#</a></h4><ul><li><p>Localization loss is computed only for predicted boxes assigned to a non-background class.</p></li><li><p>The components are:</p><ul><li><strong>Smooth L1 Loss:</strong> Applied between the target box coordinates and the predicted box coordinates.</li><li><strong>Generalized IOU Loss (GIOU Loss):</strong> Defined as $1 - \text{GIOU}$. Minimizing this loss encourages the model to increase the overlap between the predicted and matched target boxes during training.</li></ul></li></ul><h4 id=auxiliary-losses>Auxiliary Losses<a hidden class=anchor aria-hidden=true href=#auxiliary-losses>#</a></h4><ul><li><p>To enhance convergence and improve performance, auxiliary losses are incorporated during training.</p></li><li><p>Outputs from <strong>all decoder layers</strong> are utilized, with each layer&rsquo;s output being passed through the shared class and bounding box MLPs to generate predictions.<img loading=lazy src=/blogs/detr/image17.png></p></li><li><p>A separate Hungarian matching is performed for the set of predictions generated at each decoder layer.</p></li><li><p>Classification and localization losses are computed for each layer based on these assignments.</p></li><li><p>The <strong>total loss</strong> is the summation of the losses from all decoder layers.</p></li><li><p>This approach ensures that earlier decoder layers learn useful representations, which the subsequent layers refine. Note that during inference, only the final decoder layer output is used.</p></li></ul><h2 id=-references>📚 References<a hidden class=anchor aria-hidden=true href=#-references>#</a></h2><ul><li><a href=https://youtu.be/v900ZFKkWxA>&ldquo;DETR Explained | End-to-End Object Detection with Transformers | DETR Tutorial Part 1&rdquo;</a></li><li><a href=https://youtu.be/NG09OJQPWWQ>&ldquo;DETR PyTorch Implementation | DETR Tutorial Part 2&rdquo;</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:38883/tags/deep-learning/>Deep Learning</a></li><li><a href=http://localhost:38883/tags/computer-vision/>Computer Vision</a></li><li><a href=http://localhost:38883/tags/object-detection/>Object-Detection</a></li><li><a href=http://localhost:38883/tags/transformers/>Transformers</a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script src=http://localhost:38883/js/experience-timeline.js></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><style>.terminal-line-container{display:flex;justify-content:center;margin-top:5rem;margin-bottom:2rem}.terminal-line{font-family:monospace;background-color:#212121;padding:15px;border-radius:5px;color:#fafafa;display:flex;align-items:center;min-width:300px}.terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:#fafafa;animation:blink 1s step-end infinite;margin-left:4px}@keyframes blink{from,to{background-color:initial}50%{background-color:#fafafa}}.header-terminal{font-family:courier new,Courier,monospace;padding:6px 12px;color:rgba(255,255,255,.9);display:flex;align-items:center;font-size:.95rem;margin-right:16px;text-decoration:none;transition:all .2s ease;letter-spacing:-.5px}.header-terminal:hover{transform:translateY(-1px);opacity:.8}.header-terminal-text{white-space:nowrap;font-weight:600;color:rgba(255,255,255,.9)}.header-terminal-cursor{display:inline-block;width:8px;height:1.2em;background-color:rgba(255,255,255,.9);animation:blink .8s step-end infinite;margin-left:4px;opacity:.9}@media(max-width:600px){.header-terminal{font-size:.85rem;padding:4px 8px}}.footer-content{display:flex;flex-direction:row;align-items:center;justify-content:center;padding:0rem 1;gap:.5rem;text-align:center}.copyright{color:var(--secondary);font-size:.9rem}.author{color:var(--primary);font-size:1rem;font-weight:500}.entry-content .profile-mode-image img{border-radius:50%;object-fit:cover;aspect-ratio:1;border:3px solid var(--primary);transition:transform .3s ease,border-color .3s ease}.entry-content .profile-mode-image img:hover{transform:scale(1.05);border-color:var(--secondary)}</style><script>(function(){const t="cd /home/about",n=document.getElementById("terminal-text");if(!n)return;let e=0;function s(){if(e<t.length)n.textContent+=t.charAt(e),e++,setTimeout(s,120);else{const e=document.querySelector(".terminal-cursor");e&&(e.style.animation="none",e.style.backgroundColor="#fafafa")}}const o=new IntersectionObserver(e=>{e.forEach(e=>{e.isIntersecting&&(setTimeout(s,500),o.unobserve(e.target))})}),i=document.querySelector(".terminal-line");i&&o.observe(i)})();function updateTerminalPath(){const t=window.location.pathname;let e="> cd";t==="/"?e+=" /home":t.endsWith("/")?e+=" /home"+t.slice(0,-1):e+=" /home"+t;const n=document.querySelector(".header-terminal-text");if(n){let t=0;n.textContent="> ";function o(){t<e.length&&(n.textContent=e.substring(0,t+1),t++,setTimeout(o,35))}o()}const s=document.getElementById("terminal-text");if(s){s.textContent="";let t=0;function i(){t<e.length&&(s.textContent=e.substring(0,t+1),t++,setTimeout(i,120))}setTimeout(i,500)}}document.addEventListener("DOMContentLoaded",updateTerminalPath),window.addEventListener("popstate",updateTerminalPath)</script><footer class=footer><div class=footer-content><div class=copyright>© 2025</div><div class=author>Sherif Ahmed</div></div></footer></body></html>