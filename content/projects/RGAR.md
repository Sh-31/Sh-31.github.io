---
title: "Relational Group Activity Recognition"
description: "Implemented a Hierarchical Relational Network architecture for Group Activity Recognition and Retrieval, based on the ECCV 2018 paper. This implementation models inter-person relations and hierarchical temporal dynamics using Relational Layer (Graph Relational Layer). Ablation experiments were conducted to assess the impact of relation modeling, attention mechanisms (Graph Attention Operator), and hierarchical layers."
date: 2025-08-27
draft: false
weight: 3
mathjax: true
cover:
    image: "projects/Relational_Group_Activity_Recognition/cover.png"
    alt: "RGAR"
    hidden: false
projectLinks:
    repo: "https://github.com/Sh-31/Relational-Group-Activity-Recognition"
    paper: "https://openaccess.thecvf.com/content_ECCV_2018/papers/Mostafa_Ibrahim_Hierarchical_Relational_Networks_ECCV_2018_paper.pdf"
tags: ["Paper-Implementation", "ECCV 2018", "Distributed-Training", "Torch", "GNN", "CNN", "LSTM", "Graph Attention", "Spatial-Temporal", "Computer Vision"]    
---

[![Repo](https://img.shields.io/badge/github-repo-black?logo=github&style=for-the-badge&scale=2)](https://github.com/Sh-31/Relational-Group-Activity-Recognition)

[![arXiv](https://img.shields.io/badge/arXiv-1607.02643-b31b1b.svg)](https://openaccess.thecvf.com/content_ECCV_2018/papers/Mostafa_Ibrahim_Hierarchical_Relational_Networks_ECCV_2018_paper.pdf)

# Relational Group Activity Recognition

## Table of Contents
1. [Key Updates](#key-updates)
2. [Introduction](#introduction)
   - [How the Relational Layer Works](#how-the-relational-layer-works)
3. [Usage](#usage)  
   - [Clone the Repository](#1-clone-the-repository)  
   - [Install Dependencies](#2-install-the-required-dependencies)  
   - [Download Model Checkpoint](#3-download-the-model-checkpoint)  
     - [Option 1: Use Python Code](#option-1-use-python-code)  
     - [Option 2: Download Directly](#option-2-download-directly)  
4. [Dataset Overview](#dataset-overview)  
   - [Example Annotations](#example-annotations)  
   - [Train-Test Split](#train-test-split)  
   - [Dataset Statistics](#dataset-statistics)  
     - [Group Activity Labels](#group-activity-labels)  
     - [Player Action Labels](#player-action-labels)  
   - [Dataset Organization](#dataset-organization)  
   - [Dataset Download Instructions](#dataset-download-instructions)  
5. [Ablation Study](#ablation-study)  
   - [Baselines](#baselines)  
     - [Single Frame Models](#single-frame-models)  
       - [Performance Comparison (Original Paper)](#performance-comparison)  
       - [My Scores (Accuracy)](#my-scores-accuracy)  
     - [Temporal Models](#temporal-models)  
       - [Performance Comparison (Original Paper)](#performance-comparison-1)  
       - [My Scores (Accuracy)](#my-scores-accuracy-1)  
     - [Attention Models (New Baseline)](#attention-models-new-baseline)
       - [My Scores (Accuracy)](#my-scores-accuracy-2)    
       - [Confusion Matrix](#rcrg-2r-11c-conc-v1-attention-confusion-matrix)

## Key Updates

- **ResNet-50 Backbone**: Replaced VGG19 with ResNet-50 for stronger feature extraction.  
- **Ablation Studies**: Comprehensive experiments to evaluate the contribution of each model component.  
- **Test-Time Augmentation (TTA)**: Implemented to improve robustness and reliability during inference.
- **Graph Attention Operator**: Implementation for an attention-based relational layer.
- **Improved Performance**: Achieves consistently higher accuracy across all baselines compared to the original paper.  
- **Modern Implementation**: Fully implemented in **PyTorch** with support from **PyTorch Geometric**.  

---

# Introduction

Traditional pooling methods (max, average, or attention pooling) reduce dimensionality but often discard important **spatial** and **relational** details between people. The **Hierarchical Relational Network (HRN)** addresses this by introducing a **relational layer** that explicitly models interactions between individuals in a **structured relationship graph**.

<p align="center">
  <img width="512" height="512" src="https://github.com/user-attachments/assets/639cb140-a4df-4cd4-befc-2e965030723c" alt="Relational Layer Illustration"/>
</p>

## How the Relational Layer Works

1. **Graph Construction**  
   - Each person in a frame is represented as a node.  
   - People are ordered based on the top-left corner `(x, y)` of their bounding boxes (first by x, then by y if tied).  
   - Edges connect a person to their neighbors, forming **cliques** in the graph.  

2. **Initial Person Features**  
   Each person’s initial representation comes from a CNN backbone (e.g., ResNet50):  

   $$P_i^0 = \text{CNN}(I_i)$$

   where $I_i$ is the cropped image around person $i$.  

3. **Relational Update**  
  
  <p align="center">
  <img src="https://github.com/user-attachments/assets/d965ea0e-8599-4d0a-b20e-73c50fbfe6d0" alt="Graph Structure Illustration" width="750"/>
  </p>

   At relational layer $\ell$, person $i$’s updated representation is:  

   $$P_i^\ell = \sum_{j \in E_i^\ell} F^\ell(P_i^{\ell-1} \oplus P_j^{\ell-1}; \theta^\ell)$$

   - $E_i^\ell$: neighbors of person $i$ in graph $G^\ell$  
   - $\oplus$: concatenation operator  
   - $F^\ell$: shared MLP for layer $\ell$ (input size $2N_{\ell-1}$, output size $N_\ell$)  

   * This step computes pairwise relation vectors between $i$ and its neighbors, then aggregates them.  

3. **Hierarchical Stacking**  

   <p align="center">
   <img src="https://github.com/user-attachments/assets/8d8f4ea7-803c-486d-8fb7-00638445ddb7" alt="Graph Structure Illustration" width="750"/>
   </p>
  
   - Multiple relational layers are stacked, compressing person features while refining relational context.  
   - The architecture supports a variable number of people $K$ (robust to occlusions or false detections).  

4. **Scene Representation**  
   The final scene feature $S$ is obtained by pooling person features from the last relational layer:  

   $$S = P_1^L \ ▽ \ P_2^L \ ▽ \dots \ ▽ \ P_K^L$$

   where $▽$ is a pooling operator (e.g., concatenation or element-wise max pooling).  

-----
## Usage

---

### 1. Clone the Repository
```bash
git clone https://github.com/Sh-31/Relational-Group-Activity-Recognition.git
```

### 2. Install the Required Dependencies
```bash
pip3 install -r requirements.txt
```

### 3. Download the Model Checkpoint
This is a manual step that involves downloading the model checkpoint files.

#### Option 1: Use Python Code
Replace the `modeling` folder with the downloaded folder:
```python
import kagglehub

# Download latest version
path = kagglehub.model_download("sherif31/relational-group-activity-recognition/pyTorch/default")

print("Path to model files:", path)
```

#### Option 2: Download Directly
Browse and download the specific checkpoint from Kaggle:  
[Relational-Group-Activity-Recognition - PyTorch Checkpoint](https://www.kaggle.com/models/sherif31/relational-group-activity-recognition/)

-----
## Dataset Overview

The dataset was created using publicly available YouTube volleyball videos. The authors annotated 4,830 frames from 55 videos, categorizing player actions into 9 labels and team activities into 8 labels. 

### Example Annotations

![image](https://github.com/user-attachments/assets/50f906ad-c68c-4882-b9cf-9200f5a380c7)

- **Figure**: A frame labeled as "Left Spike," with bounding boxes around each player, demonstrating team activity annotations.

![image](https://github.com/user-attachments/assets/cca9447a-8b40-4330-a11d-dbc0feb230ff)

### Train-Test Split

- **Training Set**: 3,493 frames
- **Testing Set**: 1,337 frames

### Dataset Statistics

#### Group Activity Labels
| Group Activity Class | Instances |
|-----------------------|-----------|
| Right set            | 644       |
| Right spike          | 623       |
| Right pass           | 801       |
| Right winpoint       | 295       |
| Left winpoint        | 367       |
| Left pass            | 826       |
| Left spike           | 642       |
| Left set             | 633       |

#### Player Action Labels
| Action Class | Instances |
|--------------|-----------|
| Waiting      | 3,601     |
| Setting      | 1,332     |
| Digging      | 2,333     |
| Falling      | 1,241     |
| Spiking      | 1,216     |
| Blocking     | 2,458     |
| Jumping      | 341       |
| Moving       | 5,121     |
| Standing     | 38,696    |

### Dataset Organization

- **Videos**: 55, each assigned a unique ID (0–54).
- **Train Videos**: 1, 3, 6, 7, 10, 13, 15, 16, 18, 22, 23, 31, 32, 36, 38, 39, 40, 41, 42, 48, 50, 52, 53, 54.
- **Validation Videos**: 0, 2, 8, 12, 17, 19, 24, 26, 27, 28, 30, 33, 46, 49, 51.
- **Test Videos**: 4, 5, 9, 11, 14, 20, 21, 25, 29, 34, 35, 37, 43, 44, 45, 47.

### Dataset Download Instructions

1. Enable Kaggle's public API. Follow the guide here: [Kaggle API Documentation](https://www.kaggle.com/docs/api).  
2. Use the provided shell script:
```bash
  chmod 600 .kaggle/kaggle.json 
  chmod +x script/script_download_volleball_dataset.sh
  .script/script_download_volleball_dataset.sh
```
For further information about dataset, you can check out the paper author's repository:  
[link](https://github.com/mostafa-saad/deep-activity-rec)

---

## [Ablation Study](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)#:~:text=In%20artificial%20intelligence%20(AI)%2C,resultant%20performance%20of%20the%20system)

### Baselines

#### Single Frame Models:

- **B1-NoRelations:** In the first stage, Resnet50 is fine-tuned and a person is represented with 2048-d features. In the second stage, each person is connected to a shared dense layer of 128 features. The person representations (each of length 128 features) are then pooled and fed to a softmax layer for group activity classification.

- **RCRG-1R-1C:** Pretrained Resnet50 network is fine-tuned and a person is represented with 2048-d features, then a single relational layer (1R), all people in 1 clique (1C), so all-pairs relationships are learned.

- **RCRG-1R-1C-!tuned:** Same as previous variant, but Pretrained Resnet50 network without fine-tuning.

- **RCRG-2R-11C:** Close to the RCRG-1R-1C variant, but uses 2 relational layers (2R) of sizes 256 and 128. The graphs of these 2 layers are 1 clique (11C) of all people. This variant and the next ones explore stacking layers with different graph structures.

- **RCRG-2R-21C:** Same as the previous model, but the first layer has 2 cliques, one per team. The second layer is all-pairs relations (1C).

- **RCRG-3R-421C:** There relational layers (of sizes 512, 256, and 128) with clique sizes of the layers set to (4, 2, 1). The first layer has 4 cliques, with each team divided into 2 cliques.

##### Performance comparison

###### Original Paper Baselines Score

<img width="615" height="542" alt="{98F2C621-4E89-47FD-A112-A25946D611F3}" src="https://github.com/user-attachments/assets/4f7cc2f7-2b6d-472d-9555-d4d9b2de65cc" />

###### My Scores (Accuracy)

| Model | Test Acc | Test Acc TTA (4) | Paper Test ACC |
| :---- | :---: | :---: | :---: |
| B1-no-relations | 89.07% | 89.06% | 85.1% |
| RCRG-1R-1C | 89.42% | \- | 86.5% |
| RCRG-1R-1C-untuned | 80.86% | \- | 75.4% |
| RCRG-2R-11C | 89.15% | \- | 86.1% |
| RCRG-2R-21C | 89.49% | \- | 87.2% |
| RCRG-3R-421C | 88.97% | \- | 86.4% |
| **RCRG-2R-11C-conc** | **89.60%** | **89.71%** | 88.3% |
| **RCRG-2R-21C-conc** | **89.60%** | 89.60% | 86.7% |
| RCRG-3R-421C-conc | 89.23% | \- | 87.3% |

Notes:
- `-conc` postfix is used to indicate concatenation pooling instead of max-pooling.
- Used 4 transform augmentation at TTA.
  
#### Temporal Models:

- **RCRG-2R-11C-conc-temporal:** Uses 2 relational layers (2R) of sizes 256 and 128. The graphs of these 2 layers are 1 clique (11C) of all people. 

- **RCRG-2R-21C:** The first layer has 2 cliques, one per team. The second layer is all-pairs relations (1C).

##### Performance comparison

###### Original Paper Baselines Score

<img width="523" height="323" alt="{848262DC-9865-4F49-A7CA-60B08675A6B8}" src="https://github.com/user-attachments/assets/de4eb7cb-3f3c-4320-baef-c7679055a6dd" />

###### My Scores (Accuracy)

| Model | Test Acc | Test Acc TTA (3) | Paper Test ACC |
| :---- | :---: | :---: | :---: |
| B1-no-relations-temporal | 88.93% | 89.60% | \- |
| RCRG-2R-11C-conc-V1 | 90.50% | 90.73% | 89.5% |
| RCRG-2R-11C-conc-V2 | **91.55%** | 91.62% | 89.5% |
| RCRG-2R-11C-conc-V3 | 91.40% | **91.77%** | 89.5% |
| RCRG-2R-21C | \- | \- | 89.4% |

Notes:
- `Temporal`: postfix is used to indicate model work with a sequence of frames, not a frame.
- `-conc` postfix is used to indicate concatenation pooling instead of max-pooling.
- The original paper did not clearly specify where the LSTM unit should be integrated into the model.  
  To explore this, I implemented three possible variants:  
  - `V1`: LSTM **before** the relational layer → allows the relational layer to learn richer spatio-temporal features.  
  - `V2`: LSTM **after** the relational layer → enhances the relational features with temporal modeling.  
  - `V3`: LSTMs **both before and after** the relational layer → combines the strengths of V1 and V2.
- I decided to train `RCRG-2R-11C-conc` only, since it achieved the best performance in both my implementation and the paper’s results.
- I implemented `B1-no-relations-temporal` to evaluate the impact of the relational layer (This model was not included in the original paper).

#### Attention Models (new baseline):

- Uses 2 relational layers (2R). The graphs of these two layers are one clique (11C) of all players, but this time using a graph attentional operator instead of an MLP for the relational layers.

###### My Scores (Accuracy)

| Model | Test Acc | Test Acc TTA (3) | Paper Test ACC |
| :---- | :---: | :---: | :---: |
| RCRG-2R-11C-conc-V1 | 91.77% | **92.00%** | \- |

###### RCRG-2R-11C-conc-V1-Attention Confusion Matrix

<img src="https://raw.githubusercontent.com/Sh-31/Relational-Group-Activity-Recognition/refs/heads/main/experiments/attention_models/RCRG-R2-C11-conc-temporal_V1_2025_06_28_18_37/Group_Activity_RCRG-R2-C11-Conc-Temporal-Attention-TTA_Eval_On_Testset_TTA_confusion_matrix.png" alt="RCRG-2R-11C-conc-V1" >